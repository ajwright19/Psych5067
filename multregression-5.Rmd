---
title: "Multiple Regression"
output: ioslides_presentation
---


## casual relationships
- does parent SES cause better grades?
    + r(gpa, ses) = .33, b = .41
- potential confound of peer relationships
    + r(ses, peer) = .29
    + r(gpa, peer) = .37
    
## Multiple ways variables can relate
- spurious relationship
- indirect (mediation)
- moderate (interaction)
- multiple "causes"

## multiple regression model

$$ \hat{Y} = b_{0} + b_{1}X_{1} + b_{2}X_{2}+...+b_{p}X_{p}  $$


## Coefficient of Determination

$$R^2 = \frac{SS_{reggression}} {SS_{Y}} = \frac{s_{\hat{Y}}^2}{s_{Y}} $$


## GPA = SES + Peer relationships
-Can be thought of as overlapping Venn diagrams
<img src="~/Box Sync/5067 Regression Spring/5067 spring 18/R2-1.png" width="700" height="500"/>

## redundent vs non-redundent information 
<img src="~/Box Sync/5067 Regression Spring/5067 spring 18/R2.png" width="700" height="500"/>

## types of correlations
- pearson ignores all outside variables

## types of correlations
- semi-partial  
    + the extent to which the part of X1 that is independent of x2 correlates with all of Y
    
## semi-partial
<img src="~/Box Sync/5067 Regression Spring/5067 spring 18/R2-2.png"width="700" height="500"/>

## semi-partial
$$ sr = r_{y(1.2)} = \frac{r_{Y1}-r_{Y2}r_{Y12} }{\sqrt{1-r_{12}^2}} $$
$$ sr^2 = R_{Y.12}^2 - r_{Y2}^2 $$

## types of correlations
- partial  
- the extent to which the part of X1  that is independent of X2  is correlated with the part of Y that is also independent of X2 

## partial correlation
<img src="~/Box Sync/5067 Regression Spring/5067 spring 18/R2-3.png" width="500" height="500"/>

## partial correlation
$$ pr = r_{y1.2} = \frac{r_{Y1}-r_{Y2}r_{Y12} }{\sqrt{1-r_{Y2}^2}\sqrt{1-r_{12}^2}} $$

$$ sr = r_{y(1.2)} = \frac{r_{Y1}-r_{Y2}r_{Y12} }{\sqrt{1-r_{12}^2}} $$

## partial correlation

$$ pr^2 = \frac{R_{Y.12}^2 - r_{Y2}^2}{1-r_{Y2}^2} $$

$$ sr^2 = R_{Y.12}^2 - r_{Y2}^2 $$



## same t-test for semi-, partial and partial regression coefficient

## how to interpret partial regression cofficients? 
- residual in simple regression can be thought of as a measure of Y that is left over after accounting for your DV
- partial correlation can be created by:  
    1. create measure of X1 independent of X2
    2. create measure of Y independent of X2
    3. correlate new measures

## interpretting multiple regression model
$$ \hat{Y} = b_{0} + b_{1}X_{1} + b_{2}X_{2}+...+b_{p}X_{p}  $$  
- intercept is when all predictors = 0  
- regression coefficients are partial regression coefficients  
- predicted change in y for a 1 unit change in x, holding all other predictors constant  

## example
```{r, echo = FALSE, message=FALSE}
Multipleregression <- read.csv("~/Box Sync/5067 Regression Spring/5067 spring 18/Multipleregression.csv")
```

```{r}
mr.model <- lm(Stress ~ Support + Anxiety, data = Multipleregression)
summary(mr.model)
library(psych)
describe(Multipleregression$Stress)
```

## visualizing multiple regression

```{r}
library(visreg)
visreg2d(mr.model,"Support", "Anxiety", plot.type = "persp")

```


## OLS
- similar to before
$$ \hat{z}_{Y} = b_{1}^*Z_{X1} + b_{2}^*Z_{X2}$$
$$ minimize \sum (z_{Y}-\hat{z}_{Y})^2$$

## standardized partial regression coefficient
$$b_{1}^* = \frac{r_{Y1}-r_{Y2}r_{12}}{1-r_{12}^2}$$

$$b_{2}^* = \frac{r_{Y2}-r_{Y1}r_{12}}{1-r_{12}^2}$$

## Notice similarity with semi-partial correlation

$$b_{1}^* = \frac{r_{Y1}-r_{Y2}r_{12}}{1-r_{12}^2}$$



$$ sr = r_{y(1.2)} = \frac{r_{Y1}-r_{Y2}r_{Y12} }{\sqrt{1-r_{12}^2}} $$


## relationships among partial, semi- and b*  

- three different ways to represent same effect  
- all standardized  
- if predictors are not correlated, sr and pr equal  
    
## Original metric

$$b_{1} = b_{1}^*\frac{s_{Y}}{s_{X1}} $$

$$b_{1}^* = b_{1}\frac{s_{X1}}{s_{Y}} $$

## Intercept

$$b_{0} = \bar{Y} - b_{1}\bar{X_{1}} - b_{2}\bar{X_{2}} $$

## Multiple correlation R

$$ \hat{Y} = b_{0} + b_{1}X_{1} + b_{2}X_{2} $$


## Multiple correlation R

- $\hat{Y}$ is a linear combination of Xs
- $r_{Y\hat{Y}}$ = multiple correlation = R

## Multiple correlation R

$$ R = \sqrt{b_{1}^*r_{Y1} + b_{2}^*r_{Y2}} $$
$$ R^2 = {b_{1}^*r_{Y1} + b_{2}^*r_{Y2}} $$

## 
<img src="~/Box Sync/5067 Regression Spring/5067 spring 18/R2-4.png" width="700" height="500"/>

##
<img src="~/Box Sync/5067 Regression Spring/5067 spring 18/R2-5.png" width="700" height="500"/>

## sum of squares decomposition

same as before

$$  \frac{SS_{regression}}{SS_{Y}} = R^2 $$
$$  {SS_{regression}} = R^2({SS_{Y})} $$



$$  {SS_{residual}} = (1- R^2){SS_{Y}} $$

## significance tests

- R2 (omnibus)  
- Regression Coefficients  
- Increments to R2  
    
## R-squared

- Same as before  
- Adding predictors into your model will increase R2 â€“ regardless of whether or not the predictor is correlated with Y.    
- Adjusted/Shrunken R2 takes into account the number of predictors in your model  
    
## Adjusted R-squared

$$R_{A}^2 = 1 - (1 -R^2)\frac{n-1}{n-p-1} $$

## Anova table

```{r}
anova(mr.model)
```


##  

```{r}
summary(mr.model)
```


## test of individual regression coefficients

$$ H_{0}: \beta_{X}= 0 $$
$$ H_{1}: \beta_{X} \neq 0 $$

## test of individual regression coefficients

$$ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}} $$

$$ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-R_{Y\hat{Y}}^2}{n-p-1}}} \sqrt{\frac {1}{1-R_{i.jkl...p}^2}}$$

- As N increases... 
- As variance explained increases... 

## Tolerance
$$ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-R_{Y\hat{Y}}^2}{n-p-1}}} \sqrt{\frac {1}{1-R_{i.jkl...p}^2}}$$  
- what cannot be explained in Xi by other predictors  
- Large tolerance (little overlap) means standard error will be small.   
- what does this mean for including a lot of variables in your model? 

## what to include
- match population model (theoretically)  
- many variables will not bias parameter estimates but will increase degrees of freedom and standard errors, potentially

## methods for entering variables
- simultaneous
- hierarchically 

## simultaneous  
- how do you interpret the regression coefficient
- how do you interpret the fit of the model? 

## hierarchical / model comparison  
- When you want to see if the fit of one model is better than another  
- Aka incremental variance
    
## hierarchical / model comparison  
- Multiple models are calculated  
- Each predictor (or set of predictors) is assessed in terms of what it adds (in terms of variance explained) at the time it is entered   
- Order is dependent on a priori hypothesis  


##   
<img src="/Users/jackson/Box Sync/5067 Regression Spring/5067 spring 18/R2-6.png" width="700" height="500"/>

## partitioning the variance
$$R_{Y.1234...p}^2 = r_{Y1}^2 + r_{Y(2.1)}^2 + r_{Y(3.21)}^2 + r_{Y(4.321)}^2 + ...   $$

- order matters! 

## R-square change
- distributed as an F
$$ F(p.new, N - 1 - p.all) = \frac {R_{m.2}^2- R_{m.1}^2} {1-R_{m.2}^2} (\frac {N-1-p.all}{p.new}) $$
- can also be written in terms of SSresiduals


## Model comparisons
```{r}

m.1 <- lm(Stress ~ Support, data = Multipleregression)
m.2 <- lm(Stress ~ Support + Anxiety, data = Multipleregression)
anova(m.1, m.2)
```


## model comparisons
```{r}
anova(m.1)
```



## model comparisons
```{r}
anova(m.2)
```

## model comparisons
```{r, echo = FALSE}
summary(m.2)
```
## model comparisons
```{r, echo = FALSE}
summary(m.1)
```

## model comparisons

```{r}
m.0 <- lm(Stress ~ 1, data = Multipleregression)
m.1 <- lm(Stress ~ Support, data = Multipleregression)
anova(m.0, m.1)
```








## Group level multiple regression
- i.e., ANOVA models  
- Need to put numbers to our categories
- Dummy code is default

```{r, echo = FALSE}
one.way <- read.csv("~/Box Sync/5067 Regression Spring/5067 spring 18/anova.csv")
one.way$group <- one.way$IV
one.way$group <- as.factor(one.way$group)
library (broom)
```

## Group level multiple regression


```{r}
model.1 <- lm(drugs ~ group, data = one.way ) 
summary(model.1)
```
## group means
```{r, message=FALSE}
library(dplyr)
(one.way %>% 
    group_by(group) %>% 
    filter(!is.na(drugs)) %>% 
    summarise(mean(drugs)))
```

## contrasts


```{r}
contrasts(one.way$group)
Multipleregression$group <- as.factor(Multipleregression$group)
contrasts(Multipleregression$group)
```

## contrasts

```{r}
contr.treatment(4)
contr.sum(4)
```

## effects (sum) coding
```{r}
contrasts(one.way$group) <- contr.sum(3)
model.2 <- lm(drugs ~ group, data = one.way ) 
tidy(model.2)
```
## effects (sum) coding
- note: intercept is the means of means
```{r}
library(psych)
describe(one.way$drugs)
table(one.way$group)
```
- you may want to do "weighted" effect coding

## 
```{r}
anova(model.2)
```
 
 - note the df for SSregression/SSbetween
 
## effects (sum) coding

```{r}
# relevel() as a way to change which group is coded what, or: 
contrasts(one.way$group) <- contr.treatment(3, base = 2)
model.3 <- lm(drugs ~ group, data = one.way ) 
tidy(model.3)
contrasts(one.way$group) <- contr.treatment(3, base = 3)
model.4 <- lm(drugs ~ group, data = one.way ) 
tidy(model.4)
```

## Multicollinearity


## Supression 

