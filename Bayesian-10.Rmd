---
title: "Bayesian"
output: ioslides_presentation
---

## ESP Example 


## ESP Example 

- Doesn't this just seem wrong? 

## Prior beliefs are important

- cannot be integrated easily within frequentistic statistics

## The effect of observations

- For a rational person, the role of data in statistics is evidentiary    
<br>
- Beliefs after observation = Total evidence observed + Beliefs before observation  
- That is, we combine observational evidence with beliefs to obtain new beliefs: rational learning

## Some definitions 
- Suppose we have beliefs about a parameter θ (a population mean, a variance, a probability...)  
- We represent uncertainty with probability distributions   
- The uncertainty we have about θ is denoted p(θ)  
- Can express ideas like: P(35<θ<100), E(θ), etc...  
- Condition on data: Uncertainty in θ after seeing data x is p(θ∣x)

## Using probability theory to change beliefs
- The central inferential principle in Bayesian statistics is Bayes' theorem  
$$ p(A∣B)=\frac{p(B∣A) p(A)}{p(B)} $$ 
- what is the definition of a p-value? 

## How should we assess evidence?
- If uncertainty is represented by probability, then can use Bayes' theorem:    
$$ p(θ∣data) \propto {p(data∣θ)p(θ)}$$
- Beliefs after (posterior) is proportional to likelihood * prior
<br>
- prior: how sure we were that θ was true, before we observed the data D    
- likelihood: the probability that you would have observed the data, if that hypothesis were true.  (kind of like a sampling distribution)  

## Binomial example ##

$$ p(k|θ) = \binom{n}{k}θ^{k}(1-θ)^{n-k} $$

$$
p(θ) = \left\{
        \begin{array}{ll}
            1, & 0\leq θ\leq1 \\
            0, & otherwise
        \end{array}
    \right.
$$

$$ θ \sim U(0,1) $$ 

## what does this look like in practice? 
- a difficulty in all of these is to compute the posterior distribution  
- what you are doing is multiplying a probability distribution by a probability distribution (sometimes many of them)...and you get a distribution in return  
<br>
1. computationally difficult  
2. hard to summarize  

## what does this look like in practice?
- Step 1, come up with a descriptive model of the data
$$ \hat{y}=\beta_{0}+\beta_{1}X_{1}$$
- i.e. what we have been doing all semester
- Step 2, describe the variation, sigma, 
$$ y \sim norm(\hat{y},\sigma) $$

## what does this look like in practice? 
- Step 3, describe a prior distribution on the parameters  
- Could be non-informative, weak, or moderately informative depending on prior knowledge  

## what does this look like in practice?  
- Step 4, interpret posterior distribution  
- Posterior indicates credible values of the parameters $\beta_{0}, \beta_{1}, \sigma$ consistent with the data  
- distribution of parameter values, not data  
- can look at range of regression lines 'consistent with the data' as opposed to a single best estimate  

## what does this look like in practice? 
- Step 5, do a posterior prediction check  
- Many ways to do this, most typical is to plot predicted data against actual data  
- Do so by taking potential $\beta_{0}, \beta_{1}, \sigma$ values (frequency depending on posterior distribution) and randomly generate X values  
- get (average) predicted values and credible ranges around those values. See if actual data make sense given these predicted values  



## 
```{r, echo=FALSE}
alcohol1 <- read.csv("~/Box Sync/5067 Regression Spring/regression/alcohol1.txt")
```

```{r, message=FALSE}
library (brms)
options (mc.cores=parallel::detectCores ()) # Run on multiple cores

b1 <- brm(alcuse ~ peer, data=alcohol1, chains=3, iter=3000)
```

##
```{r, echo=FALSE}
summary(b1)
```


##

```{r}
lm.1 <- lm(alcuse ~ peer, data = alcohol1)
summary(lm.1)
```

##
```{r}
plot(b1)
```

##
```{r, eval=FALSE}
library(shinystan)
launch_shiny(b1) 
```

```{r}
pp_check(b1, re_formula = NA, type = "dens_overlay")
```

```{r}
pp_check(b1, type = "scatter_avg", nsamples = 100)

```


## Bayes Factor


