---
title: "Logistic Regression"
output: ioslides_presentation
---


## Logistic Regression

Used when your DV is binary (0,1)  
    - Clinical diagnosis   
    - Disease prevalence    
    - Experiences (Yes/No)    
    
## Assumption violations
    
Violates:      
    - Correctly specified form  
    - Homoscedasticity   
    - Normality of the errors  
    
## 
<img src="Log.1.png" width="750" height="550"/>

##
<img src="Log.2.png" width="750" height="550"/>

##
<img src="Log.3.png" width="750" height="550"/>

## Need to think in terms of probabilities

- If we use OLS, we violate assumptions and have predicted values that go outside 0 & 1   
- How does the predicted probability of getting a 0 or a 1 relate to our predictors?       

$$  \hat{p}_{i} \leftrightsquigarrow b_{0} + b_{1}X_{1} + b_{2}X_{2}... b_{3}X_{p} $$

## Generalized linear models 

- extend the general linear model framework
- used to describe different Data Generating Processes (DGPs) other than gausian normal
- need to use if the range of Y is restricted (e.g. binary, count) and/or the variance of Y depends on the mean  
<br>



## Generalized linear models 

- made up of two functions  
1. Link Function - describes how the mean depends on the predictors $g(\mu) = \eta_{i}$

2. Variance Function  - describes how the variance depends on the mean $var(Y)=\phi V(\mu)$

## Error structure

In  LMs, we assume that the errors $ε_{i}$ are independent and identically distributed such that
<br>
<br>
$E[ε_{i}] = 0$ and $var[ε_{i}] = s^2$
<br>
<br>
Typically we assume $ε_{i}\sim N(0,σ^2)$
as a basis for inference, e.g. t-tests

- This assumption is easy to make as the mean and variance are not related. For other types of distributions they may be related or you may have some idea about what shape the errors will take. 

## Generalized linear model

- need to accomplish two goals: 
  1. specify a link function  
  2. need to specify the error structure  
  
- luckily this often handled together with a single step

## General Linear Model

- GLM (aka regression as you know it) is a special case of Generalized Linear Models

- Link function (describes how the mean depends on the predictors) is $g(\mu) = \mu$.

- Variance Function (describes how the variance depends on the mean, related to a particular distribution) $\phi V(\mu) = 1*\sigma^2$

##  

- In some situations (not when working with a dichotomous outcome) an IV variable can be transformed to improve linearity and homogeneity of variance  
- problems:  
      + response variable has changed!  
      + transformation must simulateneously improve linearity and homogeneity of variance   
      + transformation may not be defined by the boundaries of the sample space

## Link function for logistic

- we need to map (0,1) to $(-\infty, \infty)$
- Logistic regression uses the logistic function to link the predicted probabilities to the predictors   
- Think of it as a transformation of Y-hats
$$g(\mu) = logit(\mu) = log (\frac {\mu_{i}}{1-\mu_{i}})$$

##
$$ f(x) = \frac{1}{1+e^{-X}} $$

<img src="Log.4.png" width="550" height="350"/>
 

##
$$ f(x) = \frac{1}{1+e^{-X}} $$
$$ \hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}} $$
- The form of the logistic function is still nonlinear (because probabilities can only range from 0 to 1)  
- Solution is to convert probabilities into odds 

## odds

- Odds are defined as the probability of being a case divided by the probability of being a noncase
- Not bound between 0 and 1 
- Range from 0 to infinity
- less than one is less than 50% probability
$$odds = \frac {\hat{p}}{1-\hat{p}}$$

$$ probability= \frac{\hat{odds}}{1+\hat{odds}} $$

## linear probability model

$$ f(x) = \frac{1}{1+e^{-X}} $$
$$ \hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}} $$

$$odds = \frac {\hat{p}}{1-\hat{p}}=e^{b_{0}+b_{1}X} $$
$$ logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X $$

##
$$ logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X $$

-Predicted scores are not dichotomous   
-Instead of predicting probabilities directly, we are instead predicting the log of the odds.



## other common link functions

- logit, probit, log-log, poisson, square root...  
- exponetial family of probability functions

- note, just because you have a dichotomous outcome you don't have to run a logistic regression e.g., you can run a probit regression  

## estimation  

- Maximum likelihood 
- OLS minimizes the errors, which also maximize R   
- In logistic regression we are not so lucky  
- Need to rely on Iterative procedure, ML Estimation
- Asymptotic standard errors (approximations)  
- Interpret test statistics as z’s, not t’s  
- Wald test = chi square with 1 df = z^2 when F(1, infinity)


## GLM in R

```{r, eval=FALSE}

glm(formula, family = gaussian, data, weights, subset,
    na.action, start = NULL, etastart, mustart, offset, 
    control = glm.control(...), model = TRUE,
  method = ”glm.fit”, x = FALSE, y = TRUE, contrasts = NULL, ...)
```

## Family Argument
The family argument  specifies link & variance function   
- Family specifies variance, while link specifies link  
- Families have default links, usually can leave  
  
```{r, eval=FALSE}

glm(y ~ X1+ X2 + X3 , family = binomial, data = dataset)

  binomial(link = "logit")
  gaussian(link = "identity")
  Gamma(link = "inverse")
  inverse.gaussian(link = "1/mu2")  
  poisson(link = "log")
```

## how to interpret
- b1 is the predicted change in the logit for a 1-unit change in X, holding the other predictors constant 

- For a one-unit change in X, holding other predictors constant, the odds that Y = 1 changes by $e^{b_{1}}$   

- e.g,. $b_{1}$ = .4, $e^{.4}$ = 1.49 

- for fitted values, need to use entire equation
$$ \hat{Y} = e^{b_{0}+b_{1}X_{1}}$$

- turn to probabilities by: odds/(1+odds)

## example

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(readr)
Personality_longevity <- read_csv("Personality_longevity.csv")

library(dplyr)
mortality <- Personality_longevity %>% 
  select (Time, Intelligence_Self, Intelligence_Mate) %>% 
  mutate(premature.d = cut(Time, breaks=c(-Inf, 62, Inf),
                     labels=c("premature","normal"))) %>% 
  select(-Time)
  
mortality$NOT.premature <- dplyr::recode(mortality$premature.d, normal ="1", premature="0")
  
mortality$NOT.premature <- as.numeric(as.character(mortality$NOT.premature ))
  
```

```{r}
# 1 = not premature
mortality
```

## 

```{r}

death.1 <- lm(NOT.premature ~ Intelligence_Self , data = mortality)
summary(death.1)
```

## 
```{r}

death.2 <- glm(NOT.premature ~ Intelligence_Self , data = mortality)
summary(death.2)
```

##
```{r}
anova(death.1)
```

##
```{r}
anova(death.2)
```

##
```{r}
death.3 <- glm(NOT.premature ~ Intelligence_Self,
               family = binomial, data = mortality)
summary(death.3)
```

##
```{r}
exp(1)^.08012

#For every 1 unit increase in Intelligence 
#the odds of living increase by 8%.

#prob = 
1.083417/(1+1.083417)

exp(1)^(.08012*20 + 0.28695)
 
6.615067/ (1+6.615067)

```


## probit 

```{r}
death.4 <- glm(NOT.premature ~ Intelligence_Self,
        family = binomial(link = "probit"), data = mortality)
summary(death.4)
```

## 

```{r}
both <- glm(NOT.premature ~ Intelligence_Mate +Intelligence_Self, 
                 family = binomial, data = mortality)
summary(both)
```


##

```{r}
cor(mortality$Intelligence_Self, mortality$Intelligence_Mate)
```

