<!DOCTYPE html>
<html>
<head>
  <title>fit</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'fit',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                      },

      // Author information
      presenters: [
            ]
    };
  </script>

  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

  </style>


</head>

<body style="opacity: 0">

<slides>

  <slide class="title-slide segue nobackground">
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
          </hgroup>
  </slide>

<slide class=""><hgroup><h2>How to build a regression model</h2></hgroup><article  id="how-to-build-a-regression-model">

<ul>
<li><p>What variables to include, what to leave out, etc.</p></li>
<li><p>3 standard approaches: 1. Use theory 2. Let the data decide 3. Compare models</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Prediction</h2></hgroup><article  id="prediction">

<ul>
<li>Our inferences are about populations</li>
<li><p>Want to generalize to other samples</p></li>
<li>three standard goals of regression:

<ol>
<li>Explain</li>
<li>Predict</li>
<li>Estimate</li>
</ol></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Prediction</h2></hgroup><article  id="prediction-1">

<pre class = 'prettyprint lang-r'>pred.1 &lt;- predict(model.1)</pre>

</article></slide><slide class=""><hgroup><h2>When do we have a good model?</h2></hgroup><article  id="when-do-we-have-a-good-model">

<ul>
<li>Look at fit, but fit is relative to our question at hand</li>
<li>Does it include all relevent variables (remember our assumption)?</li>
<li><p>Balance between parsimony and completeness</p></li>
<li><p>&ldquo;Building&rdquo; models up or &ldquo;pruning&rdquo; models down</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Overfitting</h2></hgroup><article  id="overfitting">

<ul>
<li><p>We donâ€™t always want the best model fit</p></li>
<li><p>This is because the model will be tuned to our particular random sample, but not other random samples</p></li>
<li><p>We are &ldquo;fitting the noise&rdquo;</p></li>
<li><p>Not only does the extra terms not help, they actually hurt!</p></li>
<li><p>This would lead to poorer prediction of new samples.</p></li>
<li><p>Our goal is to identify the true population model, not just increase fit</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Example</h2></hgroup><article  id="example">

</article></slide><slide class=""><hgroup><h2>Overfitting</h2></hgroup><article  id="overfitting-1">

<ul>
<li>Even if we have a set number of predictors we are going to use we tend ot overfit</li>
</ul>

<p>\[ \hat{Y} = b_{0} + b_{1}X + b_{2}Z + b_{3}XZ \]</p>

<p>\[ \hat{Y} = .6 + .9X + .4Z + -.24XZ \] - because we use the specifics (which include sampling error) of that sample to estimate our coefficients</p>

</article></slide><slide class=""><hgroup><h2>Relationship to p-hacking</h2></hgroup><article  id="relationship-to-p-hacking">

<ul>
<li>Making your model work ie finding the set of covariates that make a p-value fall under .05 will lead to over fitting.</li>
<li>This means that prediction will be even worse than the typical under powered experiment</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Cross validation</h2></hgroup><article  id="cross-validation">

<ul>
<li>Compare different models in their prediction in and out of sample (a test sample)</li>
<li>Identify the model with the lowest test set error (MSE)</li>
<li>Not always possible to find a large enough test sample</li>
</ul>

</article></slide><slide class=""><hgroup><h2>k-fold validation</h2></hgroup><article  id="k-fold-validation">

<ul>
<li>Samples are randomly partitioned into sets (called folds) of roughly equal size</li>
<li>model is &ldquo;trained&rdquo; on k-1 folds, and then validated with the remaining fold</li>
<li>k is usually 5-10</li>
<li>E.g., k = 10, 90% of data is used to train, 10% to predict and validate.</li>
<li>Average MSE across all of the validation folds, choose model with lowest average MSE</li>
</ul>

</article></slide><slide class=""><hgroup><h2>leave one out (LOO) validation</h2></hgroup><article  id="leave-one-out-loo-validation">

<ul>
<li>like k-fold but but k = sample size, N.</li>
<li>Run model with kth datapoint omitted.</li>
<li>Observe the average MSE or residual standard error (sigma)</li>
<li>compare the average MSE among competing models</li>
</ul>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section">

<pre class = 'prettyprint lang-r'>library(caret)
# we will use the iris dataset (included in base)
iris</pre>

<pre >##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
## 1            5.1         3.5          1.4         0.2     setosa
## 2            4.9         3.0          1.4         0.2     setosa
## 3            4.7         3.2          1.3         0.2     setosa
## 4            4.6         3.1          1.5         0.2     setosa
## 5            5.0         3.6          1.4         0.2     setosa
## 6            5.4         3.9          1.7         0.4     setosa
## 7            4.6         3.4          1.4         0.3     setosa
## 8            5.0         3.4          1.5         0.2     setosa
## 9            4.4         2.9          1.4         0.2     setosa
## 10           4.9         3.1          1.5         0.1     setosa
## 11           5.4         3.7          1.5         0.2     setosa
## 12           4.8         3.4          1.6         0.2     setosa
## 13           4.8         3.0          1.4         0.1     setosa
## 14           4.3         3.0          1.1         0.1     setosa
## 15           5.8         4.0          1.2         0.2     setosa
## 16           5.7         4.4          1.5         0.4     setosa
## 17           5.4         3.9          1.3         0.4     setosa
## 18           5.1         3.5          1.4         0.3     setosa
## 19           5.7         3.8          1.7         0.3     setosa
## 20           5.1         3.8          1.5         0.3     setosa
## 21           5.4         3.4          1.7         0.2     setosa
## 22           5.1         3.7          1.5         0.4     setosa
## 23           4.6         3.6          1.0         0.2     setosa
## 24           5.1         3.3          1.7         0.5     setosa
## 25           4.8         3.4          1.9         0.2     setosa
## 26           5.0         3.0          1.6         0.2     setosa
## 27           5.0         3.4          1.6         0.4     setosa
## 28           5.2         3.5          1.5         0.2     setosa
## 29           5.2         3.4          1.4         0.2     setosa
## 30           4.7         3.2          1.6         0.2     setosa
## 31           4.8         3.1          1.6         0.2     setosa
## 32           5.4         3.4          1.5         0.4     setosa
## 33           5.2         4.1          1.5         0.1     setosa
## 34           5.5         4.2          1.4         0.2     setosa
## 35           4.9         3.1          1.5         0.2     setosa
## 36           5.0         3.2          1.2         0.2     setosa
## 37           5.5         3.5          1.3         0.2     setosa
## 38           4.9         3.6          1.4         0.1     setosa
## 39           4.4         3.0          1.3         0.2     setosa
## 40           5.1         3.4          1.5         0.2     setosa
## 41           5.0         3.5          1.3         0.3     setosa
## 42           4.5         2.3          1.3         0.3     setosa
## 43           4.4         3.2          1.3         0.2     setosa
## 44           5.0         3.5          1.6         0.6     setosa
## 45           5.1         3.8          1.9         0.4     setosa
## 46           4.8         3.0          1.4         0.3     setosa
## 47           5.1         3.8          1.6         0.2     setosa
## 48           4.6         3.2          1.4         0.2     setosa
## 49           5.3         3.7          1.5         0.2     setosa
## 50           5.0         3.3          1.4         0.2     setosa
## 51           7.0         3.2          4.7         1.4 versicolor
## 52           6.4         3.2          4.5         1.5 versicolor
## 53           6.9         3.1          4.9         1.5 versicolor
## 54           5.5         2.3          4.0         1.3 versicolor
## 55           6.5         2.8          4.6         1.5 versicolor
## 56           5.7         2.8          4.5         1.3 versicolor
## 57           6.3         3.3          4.7         1.6 versicolor
## 58           4.9         2.4          3.3         1.0 versicolor
## 59           6.6         2.9          4.6         1.3 versicolor
## 60           5.2         2.7          3.9         1.4 versicolor
## 61           5.0         2.0          3.5         1.0 versicolor
## 62           5.9         3.0          4.2         1.5 versicolor
## 63           6.0         2.2          4.0         1.0 versicolor
## 64           6.1         2.9          4.7         1.4 versicolor
## 65           5.6         2.9          3.6         1.3 versicolor
## 66           6.7         3.1          4.4         1.4 versicolor
## 67           5.6         3.0          4.5         1.5 versicolor
## 68           5.8         2.7          4.1         1.0 versicolor
## 69           6.2         2.2          4.5         1.5 versicolor
## 70           5.6         2.5          3.9         1.1 versicolor
## 71           5.9         3.2          4.8         1.8 versicolor
## 72           6.1         2.8          4.0         1.3 versicolor
## 73           6.3         2.5          4.9         1.5 versicolor
## 74           6.1         2.8          4.7         1.2 versicolor
## 75           6.4         2.9          4.3         1.3 versicolor
## 76           6.6         3.0          4.4         1.4 versicolor
## 77           6.8         2.8          4.8         1.4 versicolor
## 78           6.7         3.0          5.0         1.7 versicolor
## 79           6.0         2.9          4.5         1.5 versicolor
## 80           5.7         2.6          3.5         1.0 versicolor
## 81           5.5         2.4          3.8         1.1 versicolor
## 82           5.5         2.4          3.7         1.0 versicolor
## 83           5.8         2.7          3.9         1.2 versicolor
## 84           6.0         2.7          5.1         1.6 versicolor
## 85           5.4         3.0          4.5         1.5 versicolor
## 86           6.0         3.4          4.5         1.6 versicolor
## 87           6.7         3.1          4.7         1.5 versicolor
## 88           6.3         2.3          4.4         1.3 versicolor
## 89           5.6         3.0          4.1         1.3 versicolor
## 90           5.5         2.5          4.0         1.3 versicolor
## 91           5.5         2.6          4.4         1.2 versicolor
## 92           6.1         3.0          4.6         1.4 versicolor
## 93           5.8         2.6          4.0         1.2 versicolor
## 94           5.0         2.3          3.3         1.0 versicolor
## 95           5.6         2.7          4.2         1.3 versicolor
## 96           5.7         3.0          4.2         1.2 versicolor
## 97           5.7         2.9          4.2         1.3 versicolor
## 98           6.2         2.9          4.3         1.3 versicolor
## 99           5.1         2.5          3.0         1.1 versicolor
## 100          5.7         2.8          4.1         1.3 versicolor
## 101          6.3         3.3          6.0         2.5  virginica
## 102          5.8         2.7          5.1         1.9  virginica
## 103          7.1         3.0          5.9         2.1  virginica
## 104          6.3         2.9          5.6         1.8  virginica
## 105          6.5         3.0          5.8         2.2  virginica
## 106          7.6         3.0          6.6         2.1  virginica
## 107          4.9         2.5          4.5         1.7  virginica
## 108          7.3         2.9          6.3         1.8  virginica
## 109          6.7         2.5          5.8         1.8  virginica
## 110          7.2         3.6          6.1         2.5  virginica
## 111          6.5         3.2          5.1         2.0  virginica
## 112          6.4         2.7          5.3         1.9  virginica
## 113          6.8         3.0          5.5         2.1  virginica
## 114          5.7         2.5          5.0         2.0  virginica
## 115          5.8         2.8          5.1         2.4  virginica
## 116          6.4         3.2          5.3         2.3  virginica
## 117          6.5         3.0          5.5         1.8  virginica
## 118          7.7         3.8          6.7         2.2  virginica
## 119          7.7         2.6          6.9         2.3  virginica
## 120          6.0         2.2          5.0         1.5  virginica
## 121          6.9         3.2          5.7         2.3  virginica
## 122          5.6         2.8          4.9         2.0  virginica
## 123          7.7         2.8          6.7         2.0  virginica
## 124          6.3         2.7          4.9         1.8  virginica
## 125          6.7         3.3          5.7         2.1  virginica
## 126          7.2         3.2          6.0         1.8  virginica
## 127          6.2         2.8          4.8         1.8  virginica
## 128          6.1         3.0          4.9         1.8  virginica
## 129          6.4         2.8          5.6         2.1  virginica
## 130          7.2         3.0          5.8         1.6  virginica
## 131          7.4         2.8          6.1         1.9  virginica
## 132          7.9         3.8          6.4         2.0  virginica
## 133          6.4         2.8          5.6         2.2  virginica
## 134          6.3         2.8          5.1         1.5  virginica
## 135          6.1         2.6          5.6         1.4  virginica
## 136          7.7         3.0          6.1         2.3  virginica
## 137          6.3         3.4          5.6         2.4  virginica
## 138          6.4         3.1          5.5         1.8  virginica
## 139          6.0         3.0          4.8         1.8  virginica
## 140          6.9         3.1          5.4         2.1  virginica
## 141          6.7         3.1          5.6         2.4  virginica
## 142          6.9         3.1          5.1         2.3  virginica
## 143          5.8         2.7          5.1         1.9  virginica
## 144          6.8         3.2          5.9         2.3  virginica
## 145          6.7         3.3          5.7         2.5  virginica
## 146          6.7         3.0          5.2         2.3  virginica
## 147          6.3         2.5          5.0         1.9  virginica
## 148          6.5         3.0          5.2         2.0  virginica
## 149          6.2         3.4          5.4         2.3  virginica
## 150          5.9         3.0          5.1         1.8  virginica</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-1">

<pre class = 'prettyprint lang-r'># define training control
train_control &lt;- trainControl(method=&quot;cv&quot;, number=10, savePredictions = TRUE)

# train the model
model.iris &lt;- train(Sepal.Length~., data=iris, trControl=train_control)</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-2">

<pre class = 'prettyprint lang-r'>print(model.iris)</pre>

<pre >## Random Forest 
## 
## 150 samples
##   4 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 136, 135, 135, 135, 135, 135, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE      
##   2     0.3327598  0.8449052  0.2724648
##   3     0.3216275  0.8538828  0.2655255
##   5     0.3185528  0.8585582  0.2582684
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 5.</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-3">

<pre class = 'prettyprint lang-r'>plot(model.iris)</pre>

<p><img src="fit-10_files/figure-html/unnamed-chunk-5-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-4">

<ul>
<li><p>Package is very flexible and will be used for more methods below</p></li>
<li><p>Use tuneGrid function to test specific parameters or models (this example was using default options that may not be useful)</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Regularization</h2></hgroup><article  id="regularization">

<ul>
<li>&ldquo;penalizing&rdquo; our model estimates to prevent overfitting</li>
<li>lasso regression is most common (ridge regression too)</li>
<li>find coefficients that compromise between (a) minimizing the SS and (b) minimizing sum of abs value of coefficients</li>
<li>Tends to &ldquo;shrink&rdquo;&quot; coefficients to zero</li>
<li>Shrinkage/penalization is based on lambda</li>
</ul>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-5">

<pre class = 'prettyprint lang-r'>library(glmnet)
library(dplyr)
    fit.ols &lt;- lm(Sepal.Length~Sepal.Width+Petal.Length+Petal.Width, data=iris)
    y &lt;- iris$Sepal.Length
    x &lt;- iris %&gt;% dplyr::select(Sepal.Width, Petal.Length,Petal.Width) %&gt;% data.matrix()
    lambdas &lt;- 10^seq(-4, 1, by = .5)
    fit.lasso &lt;-glmnet(x = x, y = y,  lambda = lambdas) </pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-6">

<pre class = 'prettyprint lang-r'>library(broom)

tidy(summary(fit.ols))</pre>

<pre >## # A tibble: 4 x 5
##   term         estimate std.error statistic  p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)     1.86     0.251       7.40 9.85e-12
## 2 Sepal.Width     0.651    0.0666      9.77 1.20e-17
## 3 Petal.Length    0.709    0.0567     12.5  7.66e-25
## 4 Petal.Width    -0.556    0.128      -4.36 2.41e- 5</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-7">

<pre class = 'prettyprint lang-r'>plot(fit.lasso)</pre>

<p><img src="fit-10_files/figure-html/unnamed-chunk-8-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>Regularization</h2></hgroup><article  id="regularization-1">

<ul>
<li>Bayesian modeling</li>
</ul>

</article></slide><slide class=""><hgroup><h2>model comparison</h2></hgroup><article  id="model-comparison">

<ul>
<li>Key to all of these approaches is the utilization of multiple models and comparing them in some mannner</li>
<li>we have done that thus far with our anova function, which compares modelâ€™s MSE using a F-test</li>
<li>There are other tests that compare models while also taking into account cross-validation and regularization techniques</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Information criteria</h2></hgroup><article  id="information-criteria">

<ul>
<li>Likelihood: The likelihood of a particular value of a parameter is the probability of obtaining the observed data if the parameter had that value. It measures how well the data supports that particular value</li>
<li>The maximum likelihood estimate of a parameter is the value of the parameter for which the probability of obtaining the observed data if the highest</li>
<li>For &ldquo;nested&rdquo; models we can compare likelihoods (via log likelihoods) with a chi-square difference test (see this with MLM soon)<br/></li>
<li>We can also use likelihoods for non-nested models to compare the best &ldquo;fit&rdquo; between models</li>
</ul>

</article></slide><slide class=""><hgroup><h2>AIC</h2></hgroup><article  id="aic">

<ul>
<li>Asymptotically equivalent to cross validation and loo<br/></li>
<li>AIC = âˆ’2 log L + 2p, where L is the maximum likelihood of the data using the model, and p is the number of parameters in the model.<br/></li>
<li>The first part is a measure of prediction error, the second a penalization</li>
<li>Lower the AIC the better<br/></li>
<li>Similar for BIC and WAIC</li>
</ul></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
