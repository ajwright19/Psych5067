<!DOCTYPE html>
<html>
<head>
  <title>Multiple Regression</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Multiple Regression',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                      },

      // Author information
      presenters: [
            ]
    };
  </script>

  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

  </style>


</head>

<body style="opacity: 0">

<slides>

  <slide class="title-slide segue nobackground">
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
          </hgroup>
  </slide>

<slide class=""><hgroup><h2>casual relationships</h2></hgroup><article  id="casual-relationships">

<ul>
<li>does parent SES cause better grades?

<ul>
<li>r(gpa, ses) = .33, b = .41</li>
</ul></li>
<li>potential confound of peer relationships

<ul>
<li>r(ses, peer) = .29</li>
<li>r(gpa, peer) = .37</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Multiple ways variables can relate</h2></hgroup><article  id="multiple-ways-variables-can-relate">

<ul>
<li>spurious relationship</li>
<li>indirect (mediation)</li>
<li>moderate (interaction)</li>
<li>multiple &ldquo;causes&rdquo;</li>
</ul>

</article></slide><slide class=""><hgroup><h2>multiple regression model</h2></hgroup><article  id="multiple-regression-model">

<p>\[ \hat{Y} = b_{0} + b_{1}X_{1} + b_{2}X_{2}+...+b_{p}X_{p}  \]</p>

</article></slide><slide class=""><hgroup><h2>Coefficient of Determination</h2></hgroup><article  id="coefficient-of-determination">

<p>\[R^2 = \frac{SS_{reggression}} {SS_{Y}} = \frac{s_{\hat{Y}}^2}{s_{Y}} \]</p>

</article></slide><slide class=""><hgroup><h2>GPA = SES + Peer relationships</h2></hgroup><article  id="gpa-ses-peer-relationships">

<p>-Can be thought of as overlapping Venn diagrams</p>

<p><img src="R2-1.png" width="700" height="500"/></p>

</article></slide><slide class=""><hgroup><h2>redundent vs non-redundent information</h2></hgroup><article  id="redundent-vs-non-redundent-information">

<p><img src="R2.png" width="700" height="500"/></p>

</article></slide><slide class=""><hgroup><h2>types of correlations</h2></hgroup><article  id="types-of-correlations">

<ul>
<li>pearson ignores all outside variables</li>
</ul>

</article></slide><slide class=""><hgroup><h2>types of correlations</h2></hgroup><article  id="types-of-correlations-1">

<ul>
<li>semi-partial

<ul>
<li>the extent to which the part of X1 that is independent of x2 correlates with all of Y</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>semi-partial</h2></hgroup><article  id="semi-partial">

<p><img src="R2-2.png"width="700" height="500"/></p>

</article></slide><slide class=""><hgroup><h2>semi-partial</h2></hgroup><article  id="semi-partial-1">

<p>\[ sr = r_{y(1.2)} = \frac{r_{Y1}-r_{Y2}r_{12} }{\sqrt{1-r_{12}^2}} \] \[ sr^2 = R_{Y.12}^2 - r_{Y2}^2 \]</p>

</article></slide><slide class=""><hgroup><h2>types of correlations</h2></hgroup><article  id="types-of-correlations-2">

<ul>
<li>partial<br/></li>
<li>the extent to which the part of X1 that is independent of X2 is correlated with the part of Y that is also independent of X2</li>
</ul>

</article></slide><slide class=""><hgroup><h2>partial correlation</h2></hgroup><article  id="partial-correlation">

<p><img src="R2-3.png" width="500" height="500"/></p>

</article></slide><slide class=""><hgroup><h2>partial correlation</h2></hgroup><article  id="partial-correlation-1">

<p>\[ pr = r_{y1.2} = \frac{r_{Y1}-r_{Y2}r_{12} }{\sqrt{1-r_{Y2}^2}\sqrt{1-r_{12}^2}} \]</p>

<p>\[ sr = r_{y(1.2)} = \frac{r_{Y1}-r_{Y2}r_{Y12} }{\sqrt{1-r_{12}^2}} \]</p>

</article></slide><slide class=""><hgroup><h2>partial correlation</h2></hgroup><article  id="partial-correlation-2">

<p>\[ pr^2 = \frac{R_{Y.12}^2 - r_{Y2}^2}{1-r_{Y2}^2} \]</p>

<p>\[ sr^2 = R_{Y.12}^2 - r_{Y2}^2 \]</p>

</article></slide><slide class=""><hgroup><h2>multiple regression model</h2></hgroup><article  id="multiple-regression-model-1">

<p>\[ \hat{Y} = b_{0} + b_{1}X_{1} + b_{2}X_{2}+...+b_{p}X_{p}  \]</p>

</article></slide><slide class=""><hgroup><h2>how to interpret multiple regression cofficients?</h2></hgroup><article  id="how-to-interpret-multiple-regression-cofficients">

<ul>
<li>residual in simple regression can be thought of as a measure of Y that is left over after accounting for your DV</li>
<li>partial correlation can be created by:

<ol>
<li>create measure of X1 independent of X2</li>
<li>create measure of Y independent of X2</li>
<li>correlate new measures</li>
</ol></li>
</ul>

</article></slide><slide class=""><hgroup><h2>interpretting multiple regression model</h2></hgroup><article  id="interpretting-multiple-regression-model">

<p>\[ \hat{Y} = b_{0} + b_{1}X_{1} + b_{2}X_{2}+...+b_{p}X_{p}  \]<br/>- intercept is when all predictors = 0<br/>- regression coefficients are &ldquo;partial&rdquo; regression coefficients<br/>- predicted change in y for a 1 unit change in x, <em>holding all other predictors constant</em></p>

</article></slide><slide class=""><hgroup><h2>example</h2></hgroup><article  id="example">

<pre class = 'prettyprint lang-r'>mr.model &lt;- lm(Stress ~ Support + Anxiety, data = Multipleregression)
summary(mr.model)</pre>

<pre >## 
## Call:
## lm(formula = Stress ~ Support + Anxiety, data = Multipleregression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.1958 -0.8994 -0.1370  0.9990  3.6995 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.31587    0.85596  -0.369 0.712792    
## Support      0.40618    0.05115   7.941 1.49e-12 ***
## Anxiety      0.25609    0.06740   3.799 0.000234 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.519 on 115 degrees of freedom
## Multiple R-squared:  0.3556, Adjusted R-squared:  0.3444 
## F-statistic: 31.73 on 2 and 115 DF,  p-value: 1.062e-11</pre>

<pre class = 'prettyprint lang-r'>library(psych)
describe(Multipleregression$Stress)</pre>

<pre >##    vars   n mean   sd median trimmed  mad  min   max range skew kurtosis
## X1    1 118 5.18 1.88   5.27    5.17 1.65 0.62 10.32  9.71 0.08     0.22
##      se
## X1 0.17</pre>

</article></slide><slide class=""><hgroup><h2>visualizing multiple regression</h2></hgroup><article  id="visualizing-multiple-regression">

<pre class = 'prettyprint lang-r'>library(visreg)
visreg2d(mr.model,&quot;Support&quot;, &quot;Anxiety&quot;, plot.type = &quot;persp&quot;)</pre>

<p><img src="multregression-5_files/figure-html/unnamed-chunk-3-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>OLS</h2></hgroup><article  id="ols">

<ul>
<li>similar to before \[ \hat{z}_{Y} = b_{1}^*Z_{X1} + b_{2}^*Z_{X2}\] \[ minimize \sum (z_{Y}-\hat{z}_{Y})^2\]</li>
</ul>

</article></slide><slide class=""><hgroup><h2>standardized partial regression coefficient</h2></hgroup><article  id="standardized-partial-regression-coefficient">

<p>\[b_{1}^* = \frac{r_{Y1}-r_{Y2}r_{12}}{1-r_{12}^2}\]</p>

<p>\[b_{2}^* = \frac{r_{Y2}-r_{Y1}r_{12}}{1-r_{12}^2}\]</p>

</article></slide><slide class=""><hgroup><h2>Notice similarity with semi-partial correlation</h2></hgroup><article  id="notice-similarity-with-semi-partial-correlation">

<p>\[b_{1}^* = \frac{r_{Y1}-r_{Y2}r_{12}}{1-r_{12}^2}\]</p>

<p>\[ sr = r_{y(1.2)} = \frac{r_{Y1}-r_{Y2}r_{Y12} }{\sqrt{1-r_{12}^2}} \]</p>

</article></slide><slide class=""><hgroup><h2>relationships among partial, semi- and b*</h2></hgroup><article  id="relationships-among-partial-semi--and-b">

<ul>
<li>three different ways to represent same effect<br/></li>
<li>all standardized<br/></li>
<li>if predictors are not correlated, sr and pr equal</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Original metric</h2></hgroup><article  id="original-metric">

<p>\[b_{1} = b_{1}^*\frac{s_{Y}}{s_{X1}} \]</p>

<p>\[b_{1}^* = b_{1}\frac{s_{X1}}{s_{Y}} \]</p>

</article></slide><slide class=""><hgroup><h2>Intercept</h2></hgroup><article  id="intercept">

<p>\[b_{0} = \bar{Y} - b_{1}\bar{X_{1}} - b_{2}\bar{X_{2}} \]</p>

</article></slide><slide class=""><hgroup><h2>How to visualize &ldquo;controlling for&rdquo;</h2></hgroup><article  id="how-to-visualize-controlling-for">

<ul>
<li>example of x and y controlling for W. Taken from <span class="cite">@nickchk</span></li>
</ul>

<ol>
<li>What is raw data?</li>
</ol>

</article></slide><slide class=""><hgroup><h2>How to visualize &ldquo;controlling for&rdquo;</h2></hgroup><article  id="how-to-visualize-controlling-for-1">

<p>2.what difference in X is explained by W. See the mean difference when W is dichotomous</p>

</article></slide><slide class=""><hgroup><h2>How to visualize &ldquo;controlling for&rdquo;</h2></hgroup><article  id="how-to-visualize-controlling-for-2">

<ol>
<li>residualize X on W. Notice the means. Run the regresion</li>
</ol>

</article></slide><slide class=""><hgroup><h2>How to visualize &ldquo;controlling for&rdquo;</h2></hgroup><article  id="how-to-visualize-controlling-for-3">

<ol>
<li>What difference in Y is explained by W.</li>
</ol>

</article></slide><slide class=""><hgroup><h2>How to visualize &ldquo;controlling for&rdquo;</h2></hgroup><article  id="how-to-visualize-controlling-for-4">

<ol>
<li>residualize Y on W. Notice the means. Run the regresion</li>
</ol>

</article></slide><slide class=""><hgroup><h2>Fit revisited</h2></hgroup><article  id="fit-revisited">

<pre class = 'prettyprint lang-r'>mr.model$R2</pre>

<pre >## NULL</pre>

</article></slide><slide class=""><hgroup><h2>Multiple correlation R</h2></hgroup><article  id="multiple-correlation-r">

<p>\[ \hat{Y} = b_{0} + b_{1}X_{1} + b_{2}X_{2} \]</p>

</article></slide><slide class=""><hgroup><h2>Multiple correlation R</h2></hgroup><article  id="multiple-correlation-r-1">

<ul>
<li>\(\hat{Y}\) is a linear combination of Xs</li>
<li>\(r_{Y\hat{Y}}\) = multiple correlation = R</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Multiple correlation R</h2></hgroup><article  id="multiple-correlation-r-2">

<p>\[ R = \sqrt{b_{1}^*r_{Y1} + b_{2}^*r_{Y2}} \] \[ R^2 = {b_{1}^*r_{Y1} + b_{2}^*r_{Y2}} \]</p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section">

<p><img src="~/Box Sync/5067 Regression Spring/Spring 19/R2-4.png" width="700" height="500"/></p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-1">

<p><img src="~/Box Sync/5067 Regression Spring/Spring 19/R2-5.png" width="700" height="500"/></p>

</article></slide><slide class=""><hgroup><h2>sum of squares decomposition</h2></hgroup><article  id="sum-of-squares-decomposition">

<p>same as before</p>

<p>\[  \frac{SS_{regression}}{SS_{Y}} = R^2 \] \[  {SS_{regression}} = R^2({SS_{Y})} \]</p>

<p>\[  {SS_{residual}} = (1- R^2){SS_{Y}} \] ## Check your model</p>

<pre class = 'prettyprint lang-r'>library(tidyverse)

fit &lt;- lm(hp ~ ., mtcars)
au &lt;- broom::augment(fit)

au.gg &lt;- au %&gt;% 
  gather(x, val, -contains(&quot;.&quot;)) %&gt;% 
  ggplot(aes(val, .resid)) +
  geom_point() +
  facet_wrap(~x, scales = &quot;free&quot;)</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-2">

<pre class = 'prettyprint lang-r'>print(au.gg)</pre>

<p><img src="multregression-5_files/figure-html/unnamed-chunk-6-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>significance tests</h2></hgroup><article  id="significance-tests">

<ul>
<li>R2 (omnibus)<br/></li>
<li>Regression Coefficients<br/></li>
<li>Increments to R2</li>
</ul>

</article></slide><slide class=""><hgroup><h2>R-squared</h2></hgroup><article  id="r-squared">

<ul>
<li>Same as before<br/></li>
<li>Adding predictors into your model will increase R2 – regardless of whether or not the predictor is correlated with Y.<br/></li>
<li>Adjusted/Shrunken R2 takes into account the number of predictors in your model</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Adjusted R-squared</h2></hgroup><article  id="adjusted-r-squared">

<p>\[R_{A}^2 = 1 - (1 -R^2)\frac{n-1}{n-p-1} \]</p>

</article></slide><slide class=""><hgroup><h2>Anova table</h2></hgroup><article  id="anova-table">

<pre class = 'prettyprint lang-r'>anova(mr.model)</pre>

<pre >## Analysis of Variance Table
## 
## Response: Stress
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Support     1 113.151 113.151  49.028 1.807e-10 ***
## Anxiety     1  33.314  33.314  14.435 0.0002336 ***
## Residuals 115 265.407   2.308                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-3">

<pre class = 'prettyprint lang-r'>summary(mr.model)</pre>

<pre >## 
## Call:
## lm(formula = Stress ~ Support + Anxiety, data = Multipleregression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.1958 -0.8994 -0.1370  0.9990  3.6995 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.31587    0.85596  -0.369 0.712792    
## Support      0.40618    0.05115   7.941 1.49e-12 ***
## Anxiety      0.25609    0.06740   3.799 0.000234 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.519 on 115 degrees of freedom
## Multiple R-squared:  0.3556, Adjusted R-squared:  0.3444 
## F-statistic: 31.73 on 2 and 115 DF,  p-value: 1.062e-11</pre>

</article></slide><slide class=""><hgroup><h2>test of individual regression coefficients</h2></hgroup><article  id="test-of-individual-regression-coefficients">

<p>\[ H_{0}: \beta_{X}= 0 \] \[ H_{1}: \beta_{X} \neq 0 \]</p>

</article></slide><slide class=""><hgroup><h2>test of individual regression coefficients</h2></hgroup><article  id="test-of-individual-regression-coefficients-1">

<p>\[ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}} \]</p>

<p>\[ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-R_{Y\hat{Y}}^2}{n-p-1}}} \sqrt{\frac {1}{1-R_{i.jkl...p}^2}}\]</p>

<ul>
<li>As N increases…</li>
<li>As variance explained increases…</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Tolerance</h2></hgroup><article  id="tolerance">

<p>\[ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-R_{Y\hat{Y}}^2}{n-p-1}}} \sqrt{\frac {1}{1-R_{i.jkl...p}^2}}\]<br/>- what cannot be explained in Xi by other predictors<br/>- Large tolerance (little overlap) means standard error will be small.<br/>- what does this mean for including a lot of variables in your model?</p>

</article></slide><slide class=""><hgroup><h2>what to include</h2></hgroup><article  id="what-to-include">

<ul>
<li>match population model (theoretically)<br/></li>
<li>many variables will not bias parameter estimates but will increase degrees of freedom and standard errors, potentially</li>
</ul>

</article></slide><slide class=""><hgroup><h2>methods for entering variables</h2></hgroup><article  id="methods-for-entering-variables">

<ul>
<li>simultaneous</li>
<li>hierarchically</li>
</ul>

</article></slide><slide class=""><hgroup><h2>simultaneous</h2></hgroup><article  id="simultaneous">

<ul>
<li>how do you interpret the regression coefficient</li>
<li>how do you interpret the fit of the model?</li>
</ul>

</article></slide><slide class=""><hgroup><h2>endogeniety discussion ADD!!</h2></hgroup><article  id="endogeniety-discussion-add">

</article></slide><slide class=""><hgroup><h2>hierarchical / model comparison</h2></hgroup><article  id="hierarchical-model-comparison">

<ul>
<li>When you want to see if the fit of one model is better than another<br/></li>
<li>Aka incremental variance</li>
</ul>

</article></slide><slide class=""><hgroup><h2>hierarchical / model comparison</h2></hgroup><article  id="hierarchical-model-comparison-1">

<ul>
<li>Multiple models are calculated<br/></li>
<li>Each predictor (or set of predictors) is assessed in terms of what it adds (in terms of variance explained) at the time it is entered<br/></li>
<li>Order is dependent on a priori hypothesis</li>
</ul>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-4">

<p><img src="/Users/jackson/Box Sync/5067 Regression Spring/5067 spring 18/R2-6.png" width="700" height="500"/></p>

</article></slide><slide class=""><hgroup><h2>R-square change</h2></hgroup><article  id="r-square-change">

<ul>
<li>distributed as an F \[ F(p.new, N - 1 - p.all) = \frac {R_{m.2}^2- R_{m.1}^2} {1-R_{m.2}^2} (\frac {N-1-p.all}{p.new}) \]</li>
<li>can also be written in terms of SSresiduals</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Model comparisons</h2></hgroup><article  id="model-comparisons">

<pre class = 'prettyprint lang-r'>m.1 &lt;- lm(Stress ~ Support, data = Multipleregression)
m.2 &lt;- lm(Stress ~ Support + Anxiety, data = Multipleregression)
anova(m.1, m.2)</pre>

<pre >## Analysis of Variance Table
## 
## Model 1: Stress ~ Support
## Model 2: Stress ~ Support + Anxiety
##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    116 298.72                                  
## 2    115 265.41  1    33.314 14.435 0.0002336 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2>model comparisons</h2></hgroup><article  id="model-comparisons-1">

<pre class = 'prettyprint lang-r'>anova(m.1)</pre>

<pre >## Analysis of Variance Table
## 
## Response: Stress
##            Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Support     1 113.15 113.151  43.939 1.12e-09 ***
## Residuals 116 298.72   2.575                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2>model comparisons</h2></hgroup><article  id="model-comparisons-2">

<pre class = 'prettyprint lang-r'>anova(m.2)</pre>

<pre >## Analysis of Variance Table
## 
## Response: Stress
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Support     1 113.151 113.151  49.028 1.807e-10 ***
## Anxiety     1  33.314  33.314  14.435 0.0002336 ***
## Residuals 115 265.407   2.308                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2>model comparisons</h2></hgroup><article  id="model-comparisons-3">

<pre >## 
## Call:
## lm(formula = Stress ~ Support + Anxiety, data = Multipleregression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.1958 -0.8994 -0.1370  0.9990  3.6995 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.31587    0.85596  -0.369 0.712792    
## Support      0.40618    0.05115   7.941 1.49e-12 ***
## Anxiety      0.25609    0.06740   3.799 0.000234 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.519 on 115 degrees of freedom
## Multiple R-squared:  0.3556, Adjusted R-squared:  0.3444 
## F-statistic: 31.73 on 2 and 115 DF,  p-value: 1.062e-11</pre>

</article></slide><slide class=""><hgroup><h2>model comparisons</h2></hgroup><article  id="model-comparisons-4">

<pre >## 
## Call:
## lm(formula = Stress ~ Support, data = Multipleregression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8215 -1.2145 -0.1796  1.0806  3.4326 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.56046    0.42189   6.069 1.66e-08 ***
## Support      0.30006    0.04527   6.629 1.12e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.605 on 116 degrees of freedom
## Multiple R-squared:  0.2747, Adjusted R-squared:  0.2685 
## F-statistic: 43.94 on 1 and 116 DF,  p-value: 1.12e-09</pre>

</article></slide><slide class=""><hgroup><h2>model comparisons</h2></hgroup><article  id="model-comparisons-5">

<pre class = 'prettyprint lang-r'>m.0 &lt;- lm(Stress ~ 1, data = Multipleregression)
m.1 &lt;- lm(Stress ~ Support, data = Multipleregression)
anova(m.0, m.1)</pre>

<pre >## Analysis of Variance Table
## 
## Model 1: Stress ~ 1
## Model 2: Stress ~ Support
##   Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)    
## 1    117 411.87                                 
## 2    116 298.72  1    113.15 43.939 1.12e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2>partitioning the variance</h2></hgroup><article  id="partitioning-the-variance">

<ul>
<li>It doesn’t make sense to ask how much variance a variable explains (unless you qualify the association)</li>
</ul>

<p>\[R_{Y.1234...p}^2 = r_{Y1}^2 + r_{Y(2.1)}^2 + r_{Y(3.21)}^2 + r_{Y(4.321)}^2 + ...   \]</p>

<ul>
<li>In other words: order matters!</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Group level multiple regression</h2></hgroup><article  id="group-level-multiple-regression">

<ul>
<li>i.e., ANOVA models<br/></li>
<li>Need to put numbers to our categories</li>
<li>Dummy code is default</li>
<li>Effect coding is an option</li>
<li>Other types too (though most are unhelpful)</li>
</ul>

</article></slide><slide class=""><hgroup><h2>When working with factors</h2></hgroup><article  id="when-working-with-factors">

<ul>
<li>know thy class</li>
</ul>

<pre class = 'prettyprint lang-r'>class(one.way$group)</pre>

<pre >## [1] &quot;factor&quot;</pre>

<pre class = 'prettyprint lang-r'>table(one.way$group)</pre>

<pre >## 
## control     tx1     tx2 
##      45     150      58</pre>

<ul>
<li>Many base R functions automatically convert character vectors to factors</li>
<li>This is okay if you are just tossing into a regression model but problematic for many uses</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Group level multiple regression</h2></hgroup><article  id="group-level-multiple-regression-1">

<pre class = 'prettyprint lang-r'>model.1 &lt;- lm(drugs ~ group, data = one.way ) 
summary(model.1)</pre>

<pre >## 
## Call:
## lm(formula = drugs ~ group, data = one.way)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.5015 -0.1524 -0.0613 -0.0613  4.0619 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.32159    0.08964   3.588 0.000402 ***
## grouptx1    -0.42402    0.10236  -4.142 4.73e-05 ***
## grouptx2    -0.33289    0.11991  -2.776 0.005923 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6013 on 247 degrees of freedom
##   (3 observations deleted due to missingness)
## Multiple R-squared:  0.06498,    Adjusted R-squared:  0.05741 
## F-statistic: 8.583 on 2 and 247 DF,  p-value: 0.0002491</pre>

</article></slide><slide class=""><hgroup><h2>What happened?</h2></hgroup><article  id="what-happened">

<ul>
<li><p>For every nominal/categorical variable that has more than 2 levels R (default R) automatically creates L-1 dummy variables</p></li>
<li><p>Each of these dummy variables consists of 0 &amp; 1s just like before, except 1 group (the reference group) only is coded as a zero</p></li>
<li><p>The interpretation of each coefficent is the difference between the group coded 1 and the reference group</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>group means</h2></hgroup><article  id="group-means">

<pre class = 'prettyprint lang-r'>library(dplyr)
(one.way %&gt;% 
    group_by(group) %&gt;% 
    filter(!is.na(drugs)) %&gt;% 
    summarise(mean(drugs)))</pre>

<pre >## # A tibble: 3 x 2
##   group   `mean(drugs)`
##   &lt;fct&gt;           &lt;dbl&gt;
## 1 control        0.322 
## 2 tx1           -0.102 
## 3 tx2           -0.0113</pre>

</article></slide><slide class=""><hgroup><h2>See what R is doing with contrasts function</h2></hgroup><article  id="see-what-r-is-doing-with-contrasts-function">

<ul>
<li>a part of every factor</li>
</ul>

<pre class = 'prettyprint lang-r'>contrasts(one.way$group)</pre>

<pre >##         tx1 tx2
## control   0   0
## tx1       1   0
## tx2       0   1</pre>

<pre class = 'prettyprint lang-r'># Can see the same with only 2 levels
contrasts(Multipleregression$group)</pre>

<pre >##         Tx
## Control  0
## Tx       1</pre>

</article></slide><slide class=""><hgroup><h2>reordering</h2></hgroup><article  id="reordering">

<ul>
<li>no inherent order, so what does R spit out at you first?</li>
<li>default is alphabetic, but what if you wanted it by another variable</li>
</ul>

<pre class = 'prettyprint lang-r'>levels(one.way$group)</pre>

<pre >## [1] &quot;control&quot; &quot;tx1&quot;     &quot;tx2&quot;</pre>

<pre class = 'prettyprint lang-r'>one.way$group.2 &lt;- relevel(one.way$group, &quot;tx2&quot;)
levels(one.way$group.2)</pre>

<pre >## [1] &quot;tx2&quot;     &quot;control&quot; &quot;tx1&quot;</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-5">

<pre class = 'prettyprint lang-r'>model.2 &lt;- lm(drugs ~ group.2, data = one.way ) 
tidy(summary(model.2))</pre>

<pre >## # A tibble: 3 x 5
##   term           estimate std.error statistic p.value
##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)     -0.0113    0.0796    -0.142 0.887  
## 2 group.2control   0.333     0.120      2.78  0.00592
## 3 group.2tx1      -0.0911    0.0937    -0.972 0.332</pre>

<pre class = 'prettyprint lang-r'>contrasts(one.way$group.2)</pre>

<pre >##         control tx1
## tx2           0   0
## control       1   0
## tx1           0   1</pre>

</article></slide><slide class=""><hgroup><h2>contrasts</h2></hgroup><article  id="contrasts">

<pre class = 'prettyprint lang-r'>## dummy variables via:
contr.treatment(4)</pre>

<pre >##   2 3 4
## 1 0 0 0
## 2 1 0 0
## 3 0 1 0
## 4 0 0 1</pre>

<pre class = 'prettyprint lang-r'>## effect coding via: 
contr.sum(4)</pre>

<pre >##   [,1] [,2] [,3]
## 1    1    0    0
## 2    0    1    0
## 3    0    0    1
## 4   -1   -1   -1</pre>

</article></slide><slide class=""><hgroup><h2>Asign contrast to factor variable</h2></hgroup><article  id="asign-contrast-to-factor-variable">

<pre class = 'prettyprint lang-r'>contr.sum(3)</pre>

<pre >##   [,1] [,2]
## 1    1    0
## 2    0    1
## 3   -1   -1</pre>

<pre class = 'prettyprint lang-r'>contrasts(one.way$group) &lt;- contr.sum(3)
model.3 &lt;- lm(drugs ~ group, data = one.way ) 
tidy(model.3)</pre>

<pre >## # A tibble: 3 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   0.0693    0.0432      1.60 0.110   
## 2 group1        0.252     0.0674      3.74 0.000228
## 3 group2       -0.172     0.0518     -3.31 0.00105</pre>

</article></slide><slide class=""><hgroup><h2>effects (sum) coding</h2></hgroup><article  id="effects-sum-coding">

<ul>
<li>note: intercept is the means of means</li>
</ul>

<pre class = 'prettyprint lang-r'>library(psych)
describe(one.way$drugs)</pre>

<pre >##    vars   n  mean   sd median trimmed mad   min  max range skew kurtosis
## X1    1 250 -0.01 0.62  -0.16   -0.15   0 -0.18 4.38  4.56 5.11    26.82
##      se
## X1 0.04</pre>

<pre class = 'prettyprint lang-r'>table(one.way$group)</pre>

<pre >## 
## control     tx1     tx2 
##      45     150      58</pre>

<ul>
<li>you may want to do &ldquo;weighted&rdquo; effect coding</li>
</ul>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-6">

<pre class = 'prettyprint lang-r'>anova(model.3)</pre>

<pre >## Analysis of Variance Table
## 
## Response: drugs
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## group       2  6.207 3.10337  8.5826 0.0002491 ***
## Residuals 247 89.312 0.36159                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

<ul>
<li>What does the ANOVA table look like for model.1 and model.2?</li>
<li>note the df for SSregression/SSbetween</li>
</ul>

</article></slide><slide class=""><hgroup><h2>what happens if you want a different reference group?</h2></hgroup><article  id="what-happens-if-you-want-a-different-reference-group">

<ul>
<li>in addition to relevel (and fct_relevel in forcats) you can change the contrast matrix</li>
</ul>

<pre class = 'prettyprint lang-r'>contrasts(one.way$group) &lt;- contr.treatment(3, base = 2)
model.4 &lt;- lm(drugs ~ group, data = one.way ) 
tidy(model.4)</pre>

<pre >## # A tibble: 3 x 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)  -0.102     0.0494    -2.07  0.0393   
## 2 group1        0.424     0.102      4.14  0.0000473
## 3 group3        0.0911    0.0937     0.972 0.332</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-7">

<pre class = 'prettyprint lang-r'>contrasts(one.way$group) &lt;- contr.treatment(3, base = 3)
model.5 &lt;- lm(drugs ~ group, data = one.way ) 
tidy(model.5)</pre>

<pre >## # A tibble: 3 x 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)  -0.0113    0.0796    -0.142 0.887  
## 2 group1        0.333     0.120      2.78  0.00592
## 3 group2       -0.0911    0.0937    -0.972 0.332</pre>

</article></slide><slide class=""><hgroup><h2>What happens if you have both nominal and continuous variables in the same model?</h2></hgroup><article  id="what-happens-if-you-have-both-nominal-and-continuous-variables-in-the-same-model">

<pre class = 'prettyprint lang-r'>model.6 &lt;- lm(drugs ~ group + alcohol, data = one.way ) 
tidy(model.6)</pre>

<pre >## # A tibble: 4 x 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)  0.00177    0.0762    0.0232 0.982     
## 2 group1       0.245      0.116     2.11   0.0356    
## 3 group2      -0.0892     0.0896   -0.995  0.321     
## 4 alcohol      0.206      0.0419    4.91   0.00000164</pre>

</article></slide><slide class=""><hgroup><h2>how should you code variables to begin with?</h2></hgroup><article  id="how-should-you-code-variables-to-begin-with">

<ul>
<li>Easy enough to work with factor variables that have their level as their name</li>
<li>No need to manually change (or create) a number associated with a level and use as.numeric</li>
<li>For simple dichotomous variables, sometimes people do code 0/1 rather than tx/control for example</li>
<li>Information could be lost without a code book, so best to name the variable what is coded 1 (e.g., tx or female rather than group or gender)</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Multicollinearity</h2></hgroup><article  id="multicollinearity">

</article></slide><slide class=""><hgroup><h2>Supression</h2></hgroup><article  id="supression"></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
