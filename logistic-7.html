<!DOCTYPE html>
<html>
<head>
  <title>Logistic Regression</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Logistic Regression',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                      },

      // Author information
      presenters: [
            ]
    };
  </script>

  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

  </style>


</head>

<body style="opacity: 0">

<slides>

  <slide class="title-slide segue nobackground">
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
          </hgroup>
  </slide>

<slide class=''><hgroup><h2>Logistic Regression</h2></hgroup><article  id="logistic-regression">

<p>Used when your DV is binary (0,1)<br/>- Clinical diagnosis<br/>- Disease prevalence<br/>- Experiences (Yes/No)</p>

</article></slide><slide class=''><hgroup><h2>Assumption violations</h2></hgroup><article  id="assumption-violations">

<p>Violates:<br/>- Correctly specified form<br/>- Homoscedasticity<br/>- Normality of the errors</p>

</article></slide><slide class=''><hgroup><h2></h2></hgroup><article  id="section">

<p><img src="Log.1.png" width="750" height="550"/></p>

</article></slide><slide class=''><hgroup><h2></h2></hgroup><article  id="section-1">

<p><img src="Log.2.png" width="750" height="550"/></p>

</article></slide><slide class=''><hgroup><h2></h2></hgroup><article  id="section-2">

<p><img src="Log.3.png" width="750" height="550"/></p>

</article></slide><slide class=''><hgroup><h2>Need to think in terms of probabilities</h2></hgroup><article  id="need-to-think-in-terms-of-probabilities">

<ul>
<li>If we use OLS, we violate assumptions and have predicted values that go outside 0 &amp; 1<br/></li>
<li>How does the predicted probability of getting a 0 or a 1 relate to our predictors?</li>
</ul>

<p>\[  \hat{p}_{i} \leftrightsquigarrow b_{0} + b_{1}X_{1} + b_{2}X_{2}... b_{3}X_{p} \]</p>

</article></slide><slide class=''><hgroup><h2>Generalized linear models</h2></hgroup><article  id="generalized-linear-models">

<ul>
<li>extend the general linear model framework<br/></li>
<li><p>need to use if the range of Y is restricted (e.g. binary, count) and/or the variance of Y depends on the mean<br/><br></p></li>
<li>made up of two functions<br/></li>
</ul>

<ol>
<li><p>Link Function - describes how the mean depends on the predictors \(g(\mu) = \eta_{i}\)</p></li>
<li><p>Variance Function - describes how the variance depends on the mean \(var(Y)=\phi V(\mu)\)</p></li>
</ol>

</article></slide><slide class=''><hgroup><h2>Error structure</h2></hgroup><article  id="error-structure">

<p>In LMs, we assume that the errors \(ε_{i}\) are independent and identically distributed such that <br> <br> \(E[ε_{i}] = 0\) and \(var[ε_{i}] = s^2\) <br> <br> Typically we assume \(ε_{i}\sim N(0,σ^2)\) as a basis for inference, e.g. t-tests</p>

</article></slide><slide class=''><hgroup><h2>Generalized linear model</h2></hgroup><article  id="generalized-linear-model">

<ul>
<li>need to accomplish two goals:</li>
</ul>

<ol>
<li>specify a link function<br/></li>
<li>need to specify the error structure</li>
</ol>

<ul>
<li>luckily this handled together</li>
</ul>

</article></slide><slide class=''><hgroup><h2>General Linear Model</h2></hgroup><article  id="general-linear-model">

<ul>
<li><p>GLM (aka regression as you know it) is a special case of Generalized Linear Models</p></li>
<li><p>Link function (describes how the mean depends on the predictors) is \(g(\mu) = \mu\).</p></li>
<li><p>Variance Function (describes how the variance depends on the mean, related to a particular distribution) \(\phi V(\mu) = 1*\sigma^2\)</p></li>
<li><p>in other words, it assumes the gaussian normal distribution</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2></h2></hgroup><article  id="section-3">

<ul>
<li>In some situations an IV variable can be transformed to improve linearity and homogeneity of variance<br/></li>
<li>problems:<br/></li>
<li>response variable has changed!<br/></li>
<li>transformation must simulateneously improve linearity and homogeneity of variance<br/></li>
<li>transformation may not be defined on the boundaries of the sample space</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Link function for logistc</h2></hgroup><article  id="link-function-for-logistc">

<ul>
<li>we need to map (0,1) to \((-\infty, \infty)\)</li>
<li>Logistic regression uses the logistic function to link the predicted probabilities to the predictors<br/></li>
<li>Think of it as a transformation of Y-hats \[g(\mu) = logit(\mu) = log (\frac {\mu_{i}}{1-\mu_{i}})\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2></h2></hgroup><article  id="section-4">

<p>\[ f(x) = \frac{1}{1+e^{-X}} \]</p>

<p><img src="Log.4.png" width="550" height="350"/></p>

</article></slide><slide class=''><hgroup><h2></h2></hgroup><article  id="section-5">

<p>\[ f(x) = \frac{1}{1+e^{-X}} \] \[ \hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}} \] - The form of the logistic function is still is nonlinear (because probabilities can only range from 0 to 1)<br/>- Solution is to convert probabilities into odds ## odds</p>

<ul>
<li>Odds are defined as the probability of being a case divided by the probability of being a noncase</li>
<li>Not bound between 0 and 1</li>
<li>Range from 0 to infinity</li>
<li>less than one is less than 50% probability \[odds = \frac {\hat{p}}{1-\hat{p}}\]</li>
</ul>

<p>\[ probability= \frac{\hat{odds}}{1+\hat{odds}} \]</p>

</article></slide><slide class=''><hgroup><h2>linear probability model</h2></hgroup><article  id="linear-probability-model">

<p>\[ f(x) = \frac{1}{1+e^{-X}} \] \[ \hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}} \]</p>

<p>\[odds = \frac {\hat{p}}{1-\hat{p}}=e^{-b_{0}+b_{1}X} \] \[ logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X \]</p>

</article></slide><slide class=''><hgroup><h2></h2></hgroup><article  id="section-6">

<p>\[ logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X \]</p>

<p>-Predicted scores are not dichotomous<br/>-Instead of predicting probabilities directly, we are instead predicting the log of the odds.</p>

</article></slide><slide class=''><hgroup><h2>other common link functions</h2></hgroup><article  id="other-common-link-functions">

<ul>
<li>logit, probit, log-log, poisson, square root&#8230;<br/></li>
<li><p>exponetial family of probability functions</p></li>
<li><p>note, just because you have a dichotomous outcome you don&#39;t have to run a logistic regression e.g., you can run a probit regression</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>estimation</h2></hgroup><article  id="estimation">

<ul>
<li>Maximum likelihood</li>
<li>OLS minimizes the errors, which also maximize R<br/></li>
<li>In logistic regression we are not so lucky<br/></li>
<li>Need to rely on Iterative procedure, ML Estimation</li>
<li>Asymptotic standard errors (approximations)<br/></li>
<li>Interpret test statistics as z’s, not t’s<br/></li>
<li>Wald test = chi square with 1 df = z2 when F(1, infinity)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>GLM in R</h2></hgroup><article  id="glm-in-r">

<pre class = 'prettyprint lang-r'>glm(formula, family = gaussian, data, weights, subset, na.action, start = NULL, etastart, mustart, offset, control = glm.control(...), model = TRUE,
method = ”glm.fit”, x = FALSE, y = TRUE, contrasts = NULL, ...)</pre>

</article></slide><slide class=''><hgroup><h2>Family Argument</h2></hgroup><article  id="family-argument">

<p>The family argument takes (the name of) a family function which specifies:<br/>1. the link function<br/>2. the variance function</p>

<pre class = 'prettyprint lang-r'>glm(y ~ X1+ X2 + X3 , family = binomial, data = dataset)

  binomial(link = &quot;logit&quot;)
  gaussian(link = &quot;identity&quot;)
  Gamma(link = &quot;inverse&quot;)
  inverse.gaussian(link = &quot;1/mu2&quot;)   poisson(link = &quot;log&quot;)</pre>

</article></slide><slide class=''><hgroup><h2>how to interpret</h2></hgroup><article  id="how-to-interpret">

<ul>
<li><p>b1 is the predicted change in the logit for a 1-unit change in X, holding the other predictors constant</p></li>
<li><p>For a one-unit change in X, holding other predictors constant, the odds that Y = 1 changes by \(e^{b_{1}}\)</p></li>
<li><p>e.g,. \(b_{1}\) = .4, \(e^{.4}\) = 1.49</p></li>
<li><p>for fitted values, need to use entire equation \[ \hat{Y} = e^{b_{0}+b_{1}X_{1}}\]</p></li>
<li><p>turn to probabilities by: odds/(1+odds)</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>example</h2></hgroup><article  id="example"></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
