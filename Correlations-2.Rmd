---
title: "Correlations"
author: "Josh Jackson"
date: "October 4, 2016"
output:
  ioslides_presentation: default
---


## Relationships

- What is the relationship between IV and DV?

  * Relationships depend a lot on measurement
  * E.g. chi-square for categorical
  
## Modeling relationships

```{r, eval=FALSE}
t.test(y ~ x, data)
```
- This semester we will examine this relationship through GLM that takes the form for y = x + e
- What analysis you use will depend on how you measure your IV (X) and your DV (Y)

## Dispersion

Variation (sum of squares)
$$ SS = {\sum{(x-\bar{x})^2}} $$
$$ SS = {\sum{(x-\mu)^2}} $$

## Dispersion
Variance
$$ s^{2} = {\frac{\sum{(x-\bar{x})^2}}{N-1}} $$
$$ \sigma^{2} = {\frac{\sum{(x-\mu)^2}}{N}} $$

## Dispersion
Standard Deviation
$$ s = \sqrt{\frac{\sum{(x-\bar{x})^2}}{N-1}} $$
$$ \sigma = \sqrt{\frac{\sum{(x-\mu)^2}}{N}} $$

## Associations
Covariation (cross products)
$$ SS = {\sum{{(x-\bar{x})(y-\bar{y})}}} $$
$$ SS = {\sum{{(x-\mu_{x})(y-\mu_{y})}}} $$

## Associations
Covariance
$$ cov_{xy}^{2} = {\frac{\sum{(x-\bar{x})(y-\bar{y})}}{N-1}} $$
$$ \sigma_{xy}^{2} = {\frac{\sum{(x-\mu_{x})(y-\mu_{y})}}{N}} $$

>- Covariance matrix is basis for many analyses
>- What are some issues that may arise when comparing covariances? 

## Associations
Correlations
$$ \rho_{xy} = {\frac{cov(X,Y)}{\sigma_{x}\sigma_{y}}} $$
$$ r_{xy} = {\frac{\sum({z_{x}z_{y})}}{N}} $$

Many other formulas exist for specific types of data, these were more helpful when we computed everything by hand (more on this later)

## Associations
Correlations

- How much two variables are linearly related
- -1 to 1 
- Invariant to changes in mean or scaling
- Most common (and basic) effect size measure

```{r,echo=FALSE, message=FALSE, cache=TRUE}
library(psych)
library(ggplot2)
data1 <- galton
```

## Associations
Correlations
```{r, echo=FALSE, cache=TRUE}
ggplot(data1, aes(x=parent, y=child)) +
    geom_point() +    
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) +     # Don't add shaded confidence region
    labs(x = "parent height", y = "child height")
```



## Z-scores
$$ z = {\frac{\sum{(x-\bar{x})}}{N}} $$

What are the characteristics of z-scores? 


## Correlations 
###Hypothesis testing

$$ H_{0}: \rho_{xy} = 0 $$
$$ H_{A}: \rho_{xy} \neq 0 $$

>- Assumes: 
>- Observations are independent
>- symmetric bivariate distribution (joint probability distribution)


----
```{r, echo=FALSE,cache=TRUE}
mu1<-0 # setting the expected value of x1
mu2<-0 # setting the expected value of x2
s11<-10 # setting the variance of x1
s12<-15 # setting the covariance between x1 and x2 
s22<-10 # setting the variance of x2
rho<-0.5 # setting the correlation coefficient between x1 and x2 
x1<-seq(-10,10,length=41) # generating the vector series x1 
x2<-x1 # copying x1 to x2
f<-function(x1,x2)
{
term1<-1/(2*pi*sqrt(s11*s22*(1-rho^2))) 
term2<--1/(2*(1-rho^2))
term3<-(x1-mu1)^2/s11
term4<-(x2-mu2)^2/s22 
term5<--2*rho*((x1-mu1)*(x2-mu2))/(sqrt(s11)*sqrt(s22))
term1*exp(term2*(term3+term4-term5))
} # setting up the function of the multivariate normal density
#
z<-outer(x1,x2,f) # calculating the density values
#
persp(x1, x2, z,
main="Joint Probability Distribution", sub=expression(italic(f)~(bold(x))==frac(1,2~pi~sqrt(sigma[11]~ sigma[22]~(1-rho^2)))~phantom(0)^bold(.)~exp~bgroup("{", list(-frac(1,2(1-rho^2)),
bgroup("[", frac((x[1]~-~mu[1])^2, sigma[11])~-~2~rho~frac(x[1]~-~mu[1], sqrt(sigma[11]))~ frac(x[2]~-~mu[2],sqrt(sigma[22]))~+~ frac((x[2]~-~mu[2])^2, sigma[22]),"]")),"}")),
col="lightgreen",
theta=30, phi=20,
r=50,
d=0.1,
expand=0.5,
ltheta=90, lphi=180,
shade=0.75,
ticktype="detailed",
nticks=5) 

# produces the 3-D plot
mtext(expression(list(mu[1]==0,mu[2]==0,sigma[11]==10,sigma[22]==10,sigma[12 ]==15,rho==0.5)), side=3) 
# adding a text line to the graph


```


## Correlations 
###Hypothesis testing

$$ H_{0}: \rho_{xy} = 0 $$
$$ H_{A}: \rho_{xy} \neq 0 $$

Test statistic

$$  t = {\frac{r}{SE_{r}}} $$
$$  t = {\frac{r}{\sqrt{\frac{1-r^{2}}{N-2}}}} $$

## Effect size 

>- The strength of relationship between two variables

>- Ω2, η2, cohen’s d, cohen’s f, hedges g, R2 , Risk-ratio, etc

>- Significance is a function of effect size and sample size

>- Statistical significance ≠ practical significance

## Effect size  
###How big is practical?

>- Cohen (.1, .3., .5)
>- Meyer & Hemphill .3 is average
>- Rosenthaul
>-
Drug TX?  | Alive |  Dead
----------|-------|--------
Treatment |  65   |  35
No Tx     |  35   |  65

>- Variance explained (more later)

##What is the size of the correlation?
Chemotherapy and breast cancer survival 
Batting ability and hit success on a single at bat 
Antihistamine use and reduced sneezing/runny nose 
Combat exposure and PTSD 
Ibuprofen on pain reduction 
Gender and weight 
Therapy and well being 
Observer ratings of attractiveness 
Gender and arm strength 
Nearness to equator and daily temperature for U.S. 

##Questions to ask yourself:
What is your N?
What is the typical effect size in the field?
Study design?
What is your DV?
Importance (reaction time vs cancer)?
Same method as IV (method variance)?

##Power calculations
```{r}
library(pwr)
pwr.r.test(n = , r = .1, sig.level = .05 , power = .8)
pwr.r.test(n = , r = .3, sig.level = .05 , power = .8)
```

##Power calculations
- But what is your confidence? 
- N = 84 gives you CI[.09, 48]
- Schönbrodt & Perugini (2013) suggest correlations stabalize at 250+ regardless of effect
- CI[.19, .39]

## Fisher’s r to z’ transformation
- If we want to make calculations based on $\rho \neq 0$ then we will run into a skewed sampling distribution
 
```{r,echo=FALSE, cache=TRUE }
r=0.95
n=50
rep=10000
z=.5*log((1+r)/(1-r))
se=1/sqrt(n-3)
zvec=rnorm(rep)*se+z
ivec=(exp(2*zvec)-1)/(exp(2*zvec)+1)
correlation <- as.data.frame(ivec)
correlation$cor <- correlation$ivec
ggplot(correlation, aes(cor)) + geom_histogram(breaks=seq(.85, .99, by = .001), aes(y = ..density..)) + geom_density()
```

## Fisher’s r to z’ transformation
- Skewed sampling distribution will rear its head when: 
    * $H_{0}: \rho \neq 0$
    * Calculating confidence intervals
    * Testing two correlations against one another
    
$$ z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}} $$

## Fisher’s r to z’ transformation
![Alt text](/Users/Jackson/Box Sync/5067 Regression Spring/Regression Lectures/fisher.png)


## How to do in R
```{r}
#rfisherz(rho)
#fisherz2r(z)
```


## Factors that influence r
1. Restriction of range
2. Very skewed distributions
3. Non-linear associations
4. Measurement overlap
5. Reliability

## Reliability


##Types of correlations
1. 

## "Homework"
1. Create a correlation matrix of the datafile XX. 
2. Is our correlation between SAT and longevity (r = .21061, N =600) significantly different than the meta-analytic estimate (.2554)?
3. Create a confidence interval around r=.30 "by hand" and using an rpackage with N = 50 and N = 500
4. Test two correlations against one another

