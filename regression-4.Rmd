---
title: "regression"
output: ioslides_presentation
---  

## Last time/this time
- Last time, we looked at associations with two continuous variables
- This time, we will extend regression framework to incorporate continuos predictors.

## We want to make eduated guesses
- E(Y|X)
- The regression of Y (DV) on X (IV) corresponds to the line that gives the mean value of Y corresponding to each possible value of X
- "Our best guess" regardless of whether our model includes categories or continuous predictor variables


## Regression Equation

$$ Y = b_{0} + b_{1}X +e  $$
$$ \hat{Y} = b_{0} + b_{1}X  $$




## OLS
- How do we find the regression estimates? 
- Ordinary Least Squares (OLS) estimation
- Minimizes deviations 

$$ min\sum(Y_{i}-\hat{Y})^{2} $$ 

- Other estimation procedures possible (and necessary in some cases)

## Regression coefficient
$$ b_{1} = \frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \frac{s_{y}}{s_{x}} $$

## Interpretation
- The regression coefficient (slope) equals the estimated change in Y for a 1-unit change in X  
- For nominal variables using dummy coding (0 and 1s) the regression coefficient equals the difference in means between the two groups 

## Standardized regression
- Regression using z-scores for Y and X
- Correlation equals standardized regression coefficent
$$ b_{1} = r_{xy} \frac{s_{y}}{s_{x}} $$
$$ r_{xy} = b_{1} \frac{s_{x}}{s_{y}} $$
$$ \beta_{yx} = b_{yx}^{*} = r_{xy} $$  

## Standardized regression equation
$$ Y = b_{1}^{*}X + e  $$
- Interpretation?

##  Raw score regression equation
- intercept serves to adjust for differences in means between X and Y

$$ \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X}) $$
- if standardized, intercept drops out  
- otherwise, intercept is where regression line crosses the y-axis at X = 0
- Also, notice that when X = $\bar{X}$ the regression line  goes through  $\bar{Y}$


## Example
```{r,echo=FALSE, message=FALSE, cache=TRUE}
library(psych)
library(ggplot2)
galton.data <- galton
```

```{r}
fit.1 <- lm(parent ~ child, data = galton.data)
anova(fit.1)
summary(fit.1)
library(psych)
describe(galton.data$parent)
```


## 

```{r, echo=FALSE, cache=TRUE}
ggplot(galton.data, aes(x=child, y=parent)) +
    geom_point() +    
    geom_smooth(method=loess, formula = y ~ x)
```

## ANOVA table
```{r}
anova(fit.1)
```



## Exploring the lm object

```{r, warning=FALSE}
library(broom)
galton.data.1 <- augment(fit.1, galton.data)
head(galton.data.1)
```


## fitted values, Y-hats
```{r}
psych::describe(galton.data.1$.fitted)      
```

```{r}
psych::describe(galton.data.1$parent)
```


## residuals

$$ \epsilon \sim N(0,\sigma) $$ 

```{r}
psych::describe(galton.data.1$.resid)
psych::describe(galton.data.1$parent)
```
- variation that is left over in Y, after accounting for X

## residuals not assocaited with X

```{r, cache=TRUE}
ggplot(galton.data.1, aes(x=.resid, y=child)) +
    geom_point() +    
    geom_smooth(method=lm)
```

## and yhat
```{r, cache=TRUE}
ggplot(galton.data.1, aes(x=.resid, y=.fitted)) +
    geom_point() +    
    geom_smooth(method=lm)
```

## r(fitted and X) = 1
```{r, cache=TRUE}
ggplot(galton.data.1, aes(x=.fitted, y=child)) +
    geom_point() +    
    geom_smooth(method=lm)
```

## Formal Description

$$ Y_{i} \sim \mathcal{N}(\mu,\,\sigma)\  $$
In english: Our DV for individual i is distributed normally with a mean of mu 
and a standard deviation of sigma

- sigma describes the spread of data around our prediction line (how close data points are to line/how small our residuals are)

## Statistical Inference
- The way the world is = our model + error
- How good is our model? Does it "fit" the data well? 

## Partitioning variance in Y
- Consider the case with no correlation btw X and Y
$$ \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X}) $$
$$ \hat{Y} = \bar{Y} $$

## Partitioning variance in Y
- To the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction

$$ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 $$

- SS total = SS regression + SS residual (or error)


## Coefficient of Determination

$$ \frac{s_{regression}^2}{s_{y}^2} = \frac{SS_{regression}}{SS_{Y}} = R^2 $$



## Example
```{r}

summary(fit.1)$r.squared
```


## Example

```{r}
cor.test(galton.data$parent, galton.data$child)
```

## calculating R2
```{r, echo=FALSE}

anova(fit.1)

```

## Computing Sum of Squares

$$  \frac{SS_{regression}}{SS_{Y}} = R^2 $$
$$  {SS_{regression}} = R^2({SS_{Y})} $$
$$  {SS_{residual}} = SS_{Y} - R^2({SS_{Y})} $$

$$  {SS_{residual}} = (1- R^2){SS_{Y}} $$

## Mean square error (MSE)
- unbiased estimate of error variance
- measure of discrepancy between the data and the model
- the MSE is the variance around the fitted regression line

## residual standard error
MSE = 2.52 
```{r, echo=FALSE}
summary(fit.1)
```


## residual standard error/deviation 

- aka standard deviation of the residual
- aka standard error of the estimate

$$ \hat{\sigma} $$

- Interpreted in original units (cf R2)
- Standard deviation of Y not accounted by model


## residual standard error/deviation (sigma)


```{r}
summary(fit.1)$sigma 
psych::describe(galton.data.1$.resid)
```


## residual standard error/deviation (sigma)

```{r}
summary(fit.1)$sigma 
psych::describe(galton.data$parent)
```

## Why do we care? Let's simulate to find out

```{r}
x.1 <- rnorm(1000, 0, 1)
e.1 <- rnorm(1000, 0, 1)
y.1 <- .5 + .55 * x.1 + e.1
d.1 <- data.frame(x.1,y.1)
m.1 <- lm(y.1 ~ x.1, data = d.1)
```
##

```{r}
summary(m.1)

```

##

```{r, echo=FALSE, warning=FALSE}
library(ggplot2)
ggplot(d.1, aes(x = x.1,y =  y.1)) +
    geom_point() +    
    geom_smooth(method = lm) +
   scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-6, 6))
```

##

```{r}
x.2 <- rnorm(1000, 0, 1)
e.2 <- rnorm(1000, 0, 2)
y.2 <- .5 + .55 * x.2 + e.2
d.2 <- data.frame(x.2,y.2)
m.2 <- lm(y.2 ~ x.2, data = d.2)
```

##
```{r}
summary(m.2)

```

##
```{r,echo=FALSE, warning=FALSE}
ggplot(d.1, aes(x=x.2, y=y.2)) +
    geom_point() +    
    geom_smooth(method=lm) +
  scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-6, 6))
```

## r2 and residual standard deviation
- two sides of same coin
- one in original units, the other standardized 
- R2 can be tricky because the numerator and denominator can be changed in different ways. 
- For example if variance in Y is changed but with the same model and residual standard error R2 could decline or increase


## standard errors for b
- represent our uncertainty (noise) in our estimate of the regression coefficient 
- different from residual standard error/deviation (but proportional to). 
- much like previously we can take our estimate (b) and put confidence regions around it to get an estimate of what could be "possible" if we ran the study again
- (see equation later)


## Inferential tests
## Omnibus test

$$ H_{0}: \rho_{XY}^2= 0 $$
$$ H_{1}: \rho_{XY}^2 \neq 0 $$

$$ F = \frac{MS_{regression}}{MS_{residial}} $$

## model comparison

- Last semester you looked at different models to see how their F and SSs changed depending on what predictors were included. We can do the same thing with regression models!   

- The basic idea is asking how much variance remains unexplained in our model. This "left over" variance can be contrasted with an alternative model/hypothesis. We can ask does adding a new predictor variable help explain more variance or should we stick wtih a parsimonious model.   

- Every model test you do implicitly implies you favoring that over an alternative model, typically the null. This framework allows you to be more flexible and explicit.   

##
```{r}
fit.1 <- lm(parent ~ child, data = galton.data)
fit.0 <- lm(parent ~ 1, data = galton)
```


## 
```{r}
summary(fit.0)
```

##
```{r}
summary(fit.1)
```


## 
```{r}
anova(fit.0)
```

##
```{r}
anova(fit.1)
```


##
```{r}
anova(fit.1, fit.0)
```

## model comparisons 

- Model comparisons are redundent with nil/null hypotheses and coefficient tests right now, but will be more flexible down the road. 
- Key is to start thikning about your implicit alternative models
- The ultimate goal would be to create two models that represent two equally plausible theories. 
- Theory A is made up of components XYZ, whereas theory B has QRS. You can then ask which theory(model) is better? 

## regression coefficient

$$ H_{0}: \beta_{XY}= 0 $$
$$ H_{1}: \beta_{XY} \neq 0 $$

## What does the regression coefficient test?
- Does X provide any predictive information? 
- Does X provide any explanatory power regarding the variability of Y? 
- Is the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)
- Is the regression line flat? 
- Are X and Y correlated?  

## Regression coefficient
$$ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}} $$
$$ t(n-2) = \frac{b_{yx}}{se_{b}} $$
** what is standardized equation? 

## Intercept

- same idea, more complex se calculation as the calculation depends on how far the X value (here zero) is away from the mean of X
- farther from the mean, less information, thus more uncertainty 
- We will come back and see this equation later

## Confidence interval for coefficents

- same equation as we've been working with
- estimate plus minus 1.96*se

## Confidence Bands for regression line
```{r, echo=FALSE, message=FALSE}
ggplot(galton.data, aes(x=child, y=parent)) +
    geom_smooth(method=lm,   # Add linear regression line
                se=TRUE) +     # Don't add shaded confidence region
    labs(x = "child height", y = "parent height")
```

## Confidence Bands
- Compare mean estimate for height of 70 based on regression vs binning

```{r, echo=FALSE}
ggplot(galton.data, aes(x=child, y=parent)) +
      geom_point() +   
  geom_smooth(method=lm,   # Add linear regression line
                se=TRUE) 
```

## Confidence Bands

$$ \hat{Y}\pm t_{critical} * se_{residual}*\sqrt{\frac {1}{n}+\frac{(X-\bar{X}bar)^2}{(n-1)s_{X}^2}}  $$


## Prediction
- very similar to confidence bands around regressions line
- differences is that is predicting and individual i's score, not the Y hat for a particular level of X. (A new Yi given x, rather than Ybar given x)
- Because there is greater variation in predicting an individual value rather than a collection of individual values (ie the mean) the prediction band is greater
- Combines unknown variability in 1) the estimated mean (as reflected in se of b) 2) peoples scores around  mean (residual standard error) 

$$ \hat{Y}\pm t_{critical} * se_{residual}*\sqrt{1+ \frac {1}{n}+\frac{(X-\bar{X}bar)^2}{(n-1)s_{X}^2}}  $$

##

```{r, warning=FALSE}
temp_var <- predict(fit.1, interval="prediction")
new_df <- cbind(galton.data, temp_var)
pred <- ggplot(new_df, aes(x=child, y=parent))+
       geom_point() +   
  geom_smooth(method=lm,se=TRUE) +
 geom_ribbon(aes(ymin = lwr, ymax = upr), 
               fill = "blue", alpha = 0.1)
```


##
```{r}
pred
```








