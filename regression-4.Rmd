---
title: "regression"
output: ioslides_presentation
---


## What is a regression equation? 
Functional relationship, ideally like a physical law

Uncovered through measurement




## Expected value of Y given X
- E(Y|X)
- The regression of Y (DV) on X (IV) corresponds to the line that gives the mean value of Y corresponding to each possible value of X

- "Our best guess"

## What is our best guess if we...
- Didn't collect any data? 
- No correlation? 
- Positive association?
- Negative association?

## Regression Equation

$$ Y = b_{0} + b_{1}X +e  $$
$$ \hat{Y} = b_{0} + b_{1}X  $$
$$ \mu_{Y} = \beta_{0} + \beta_{1}X + \epsilon   $$

## Regression terms
- Y/DV/Outcome/Response/Criterion
- X/IV/Predictor/Explanatory variable
- Regression coeffiecent (weight)/b/b*/$\beta$
- Intercept bo/$\beta_{0}$
- Error/Residuals

## OLS
- Ordinary Least Squares (OLS) estimation
- Minimizes deviations 

$$ min\sum(Y_{i}-\hat{Y})^{2} $$ 

- Other estimation procedures possible (and necessary in some cases)

## Regression coefficient
$$ b_{yx} = \frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \frac{s_{y}}{s_{x}} $$

## Interpretation
The slope equals the estimated change in Y for a 1-unit change in X

## Standardized regression
- Regression using z-scores for Y and X
- Correlation equals standardized regression coefficent
$$ b_{yx} = r_{xy} \frac{s_{y}}{s_{x}} $$
$$ r_{xy} = b_{yx} \frac{s_{x}}{s_{y}} $$
$$ \beta_{yx} = b_{yx}^{*} = r_{xy} $$  

## Standardized regression equation
$$ Y = b_{1}^{*}X + e  $$
- Interpretation?

##  Raw score regression equation
- intercept serves to adjust for differences in means between X and Y

$$ \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X}) $$
- if standardized, intercept drops out  
- otherwise, intercept is where regression line crosses the y-axis at X = 0


## Example
```{r,echo=FALSE, message=FALSE, cache=TRUE}
library(psych)
library(ggplot2)
galton.data <- galton
```

```{r}
fit.1 <- lm(parent ~ child, data = galton.data)
summary(fit.1)
```

## ANOVA table
```{r}
anova(fit.1)
```


## lm objects

```{r, eval=FALSE}
coefficients(fit.1)       # coefficients
residuals(fit.1)          # residuals
fitted.values(fit.1)      # fitted values
summary(fit.1)$r.squared  # R-sq for the model
summary(fit.1)$sigma      # se of the model
```

## lm objects
The way R handles model objects is a little cumbersome as they are not data frames. You can see your output and maybe do some basic descriptives but you wont be able to do advanced manipulation 
```{r}
head(resid(fit.1))
```

## lm objects 

If you want to use your model results/objects later on you (hint, we will) you need to turn them into an easier to use form

```{r}
# broom package (found in the tidyverse package)
library(broom)
fit.1.data <- tidy(fit.1) #tidy turns the summary into a dataframe 
fit.1.data
```

## lm objects
augment ammends the original dataset with all the lm objects. The names of the objects are different than a normal lm object, namely they have a "."" infront of the name
```{r}
library(broom)
galton.data.1 <- augment(fit.1, galton.data)
head(galton.data.1)
```


## fitted values, Y-hats
```{r}
library(psych)

describe(galton.data.1$.fitted)      
```
```{r}
describe(galton.data.1$parent)
```

##  Correlation

```{r}
cor.test(galton.data.1$parent, galton.data.1$.fitted)
```


##residuals

```{r}
head(galton.data.1$.resid)
describe(galton.data.1$.resid)
describe(galton.data.1$parent)
```
- variation that is left over in Y, after accounting for X

## Statistical Inference
- The way the world is = our model + error
- How good is our model? Does it "fit" the data well? 

## Partitioning variance in Y
- Consider the case with no correlation btw X and Y
$$ \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X}) $$
$$ \hat{Y} = \bar{Y} $$

## Partitioning variance in Y
- To the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction

## Partitioning variance in Y into 3 components
$$ Y = \hat{Y} + e$$
$$ Y = \hat{Y} + (Y - \hat{Y}) $$
$$ Y - \bar{Y} = (\hat{Y} -\bar{Y}) + (Y - \hat{Y}) $$
$$ (Y - \bar{Y})^2 = [(\hat{Y} -\bar{Y}) + (Y - \hat{Y})]^2 $$
$$ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 $$

## Partitioning the variation in Y
$$ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 $$

- SS total = SS regression + SS residual (or error)

## Partitioning the variation for ANOVA
$$ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 $$
- SS total = SS between + SS within

## Similarity with ANOVA\

## Partitioning variation in Y
$$ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 $$
$$ s_{y}^2 = s_{regression}^2 + s_{residual}^2 $$
$$ 1 = \frac{s_{regression}^2}{s_{y}^2} + \frac{s_{residual}^2}{s_{y}^2}  $$


## Coefficient of Determination

$$ \frac{s_{regression}^2}{s_{y}^2} = \frac{SS_{regression}}{SS_{Y}} = R^2 $$


## Example
```{r}

summary(fit.1)

```

##Example

```{r}
cor.test(galton.data$parent, galton.data$child)
```

## calculating R2
```{r, echo=FALSE}

anova(fit.1)

```

## Computing Sum of Squares

$$  \frac{SS_{regression}}{SS_{Y}} = R^2 $$
$$  {SS_{regression}} = R^2({SS_{Y})} $$
$$  {SS_{residual}} = SS_{Y} - R^2({SS_{Y})} $$

$$  {SS_{residual}} = (1- R^2){SS_{Y}} $$

## Mean square error (MSE)
- unbiased estimate of error variance
- measure of discrepancy between the data and the model
- the MSE is the variance around the fitted regression line

## MSE and residual standard error

MSE = 2.52
```{r, echo=FALSE}
summary(fit.1)
```


## residual standard error (sigma)

```{r}
describe(galton.data$parent)
```

## residual standard error (sigma)

- aka standard deviation of the residual
- aka standard error of the estimate

- Interpreted in original units (cf R2)
- Standard deviation of Y not accounted by model
- MSE is related to sigma, sqrt[(SSE)/df(error)] = sigma 
- (note df might be off slightly because of unbiasing)


## residual standard error (sigma)

```{r}
head(galton.data.1)
describe(galton.data.1$.resid)

```

## Omnibus test

$$ H_{0}: \rho_{XY}^2= 0 $$
$$ H_{1}: \rho_{XY}^2 \neq 0 $$

$$ F = \frac{MS_{regression}}{MS_{residial}} $$

## Regression coefficient

$$ H_{0}: \beta_{XY}= 0 $$
$$ H_{1}: \beta_{XY} \neq 0 $$

## What does the regression coefficient test?
- Does X provide any predictive information? 
- Does X provide any explanatory power regarding the variability of Y? 
- Is the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)
- Is the regression line flat? 
- Are X and Y correlated?  

## Regression coefficient
$$ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}} $$
$$ t(n-2) = \frac{b_{yx}}{se_{b}} $$
** what is standardized equation? 

## Intercept

- same idea, more complex se calculation

## Confidence interval for coefficents

- same equation as we've been working with

## Confidence Bands for regression line
```{r, echo=FALSE, message=FALSE}
library(ggplot2)
ggplot(galton.data, aes(x=parent, y=child)) +
    geom_smooth(method=lm,   # Add linear regression line
                se=TRUE) +     # Don't add shaded confidence region
    labs(x = "parent height", y = "child height")
```

## Confidence Bands
- Compare mean estimate for height of 70 based on regression vs binning

```{r, echo=FALSE}
library(ggplot2)
ggplot(galton.data, aes(x=parent, y=child)) +
      geom_point() +   
  geom_smooth(method=lm,   # Add linear regression line
                se=TRUE) 
```

## Confidence Bands

$$ \hat{Y}\pm t_{critical} * se_{residual}\sqrt{\frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}  $$


## Prediction

```{r}

```

- More later when we start Bayesian modeling
- A new Y given x, rather than Ybar given x. 


## Coding of qualitative variables

- Dummy coding
- Effects





