<!DOCTYPE html>
<html>
<head>
  <title>Regression</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />



  <meta name="date" content="2017-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Regression',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                      },

      // Author information
      presenters: [
            {
        name:  'Josh Jackson' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            ]
    };
  </script>

  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

  </style>


</head>

<body style="opacity: 0">

<slides>

  <slide class="title-slide segue nobackground">
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
            <p style="margin-top: 6px; margin-left: -2px;">February 19, 2017</p>
          </hgroup>
  </slide>

<slide class=''><hgroup><h2>What is a regression equation?</h2></hgroup><article  id="what-is-a-regression-equation">

<p>Functional relationship, ideally like a physical law</p>

<p>Uncovered through measurement</p>

</article></slide><slide class=''><hgroup><h2>Scatter Plot with best fit line</h2></hgroup><article  id="scatter-plot-with-best-fit-line">

<p><img src="Regression-3_files/figure-html/unnamed-chunk-2-1.png" width="720" /></p>

</article></slide><slide class=''><hgroup><h2>Expected value of Y given X</h2></hgroup><article  id="expected-value-of-y-given-x">

<ul>
<li>E(Y|X)</li>
<li><p>The regression of Y (DV) on X (IV) corresponds to the line that gives the mean value of Y corresponding to each possible value of X</p></li>
<li><p>&quot;Our best guess&quot;</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>What is our best guess if we&#8230;</h2></hgroup><article  id="what-is-our-best-guess-if-we...">

<ul>
<li>Didn&#39;t collect any data?</li>
<li>No correlation?</li>
<li>Positive association?</li>
<li>Negative association?</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Regression Equation</h2></hgroup><article  id="regression-equation">

<p>\[ Y = b_{0} + b_{1}X +e  \] \[ \hat{Y} = b_{0} + b_{1}X  \] \[ \mu_{Y} = \beta_{0} + \beta_{1}X + \epsilon   \]</p>

</article></slide><slide class=''><hgroup><h2>Regression terms</h2></hgroup><article  id="regression-terms">

<ul>
<li>Y/DV/Outcome/Response/Criterion</li>
<li>X/IV/Predictor/Explanatory variable</li>
<li>Regression coeffiecent (weight)/b/b*/\(\beta\)</li>
<li>Intercept bo/\(\beta_{0}\)</li>
<li>Error/Residuals</li>
</ul>

</article></slide><slide class=''><hgroup><h2>OLS</h2></hgroup><article  id="ols">

<ul>
<li>Ordinary Least Squares (OLS) estimation</li>
<li>Minimizes deviations</li>
</ul>

<p>\[ min\sum(Y_{i}-\hat{Y})^{2} \]</p>

<ul>
<li>Other estimation procedures possible (and necessary in some cases)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Regression coefficient</h2></hgroup><article  id="regression-coefficient">

<p>\[ b_{yx} = \frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \frac{s_{y}}{s_{x}} \]</p>

</article></slide><slide class=''><hgroup><h2>Interpretation</h2></hgroup><article  id="interpretation">

<p>The slope equals the estimated change in Y for a 1-unit change in X</p>

</article></slide><slide class=''><hgroup><h2>Standardized regression</h2></hgroup><article  id="standardized-regression">

<ul>
<li>Regression using z-scores for Y and X</li>
<li>Correlation equals standardized regression coefficent \[ b_{yx} = r_{xy} \frac{s_{y}}{s_{x}} \] \[ r_{xy} = b_{yx} \frac{s_{x}}{s_{y}} \] \[ \beta_{yx} = b_{yx}^{*} = r_{xy} \]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Standardized regression equation</h2></hgroup><article  id="standardized-regression-equation">

<p>\[ Y = b_{1}^{*}X + e  \] - Interpretation?</p>

</article></slide><slide class=''><hgroup><h2>Raw score regression equation</h2></hgroup><article  id="raw-score-regression-equation">

<ul>
<li>intercept serves to adjust for differences in means between X and Y</li>
</ul>

<p>\[ \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X}) \] - if standardized, intercept drops out<br/>- otherwise, intercept is where regression line crosses the y-axis at X = 0</p>

</article></slide><slide class=''><hgroup><h2>Example</h2></hgroup><article  id="example">

<pre class = 'prettyprint lang-r'>fit.1 &lt;- lm(parent ~ child, data = galton.data)
summary(fit.1)</pre>

<pre >## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***
## child        0.32565    0.02073   15.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105, Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16</pre>

</article></slide><slide class=''><hgroup><h2>ANOVA table</h2></hgroup><article  id="anova-table">

<pre class = 'prettyprint lang-r'>anova(fit.1)</pre>

<pre >## Analysis of Variance Table
## 
## Response: parent
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## child       1  623.26  623.26  246.84 &lt; 2.2e-16 ***
## Residuals 926 2338.10    2.52                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=''><hgroup><h2>lm objects</h2></hgroup><article  id="lm-objects">

<pre class = 'prettyprint lang-r'>coefficients(fit.1)       # coefficients
residuals(fit.1)          # residuals
fitted.values(fit.1)      # fitted values
summary(fit.1)$r.squared  # R-sq for the model
summary(fit.1)$sigma      # se of the model</pre>

</article></slide><slide class=''><hgroup><h2>lm objects</h2></hgroup><article  id="lm-objects-1">

<p>The way R handles model objects is a little cumbersome as they are not data frames. You can see your output and maybe do some basic descriptives but you wont be able to do advanced manipulation</p>

<pre class = 'prettyprint lang-r'>head(resid(fit.1))</pre>

<pre >##          1          2          3          4          5          6 
##  4.2721996  2.2721996 -0.7278004 -1.7278004 -2.2278004  1.1093758</pre>

</article></slide><slide class=''><hgroup><h2>lm objects</h2></hgroup><article  id="lm-objects-2">

<p>If you want to use your model results/objects later on you (hint, we will) you need to turn them into an easier to use form</p>

<pre class = 'prettyprint lang-r'># broom package (found in the tidyverse package)
library(broom)
fit.1.data &lt;- tidy(fit.1) #tidy turns the summary into a dataframe 
fit.1.data</pre>

<pre >##          term   estimate std.error statistic       p.value
## 1 (Intercept) 46.1353499 1.4122473  32.66804 2.526465e-156
## 2       child  0.3256475 0.0207272  15.71112  1.732509e-49</pre>

</article></slide><slide class=''><hgroup><h2>lm objects</h2></hgroup><article  id="lm-objects-3">

<p>augment ammends the original dataset with all the lm objects. The names of the objects are different than a normal lm object, namely they have a &quot;.&quot;&quot; infront of the name</p>

<pre class = 'prettyprint lang-r'>library(broom)
galton.data.1 &lt;- augment(fit.1, galton.data)
head(galton.data.1)</pre>

<pre >##   parent child  .fitted   .se.fit     .resid        .hat   .sigma
## 1   70.5  61.7 66.22780 0.1423187  4.2721996 0.008021794 1.583599
## 2   68.5  61.7 66.22780 0.1423187  2.2721996 0.008021794 1.588097
## 3   65.5  61.7 66.22780 0.1423187 -0.7278004 0.008021794 1.589686
## 4   64.5  61.7 66.22780 0.1423187 -1.7278004 0.008021794 1.588844
## 5   64.0  61.7 66.22780 0.1423187 -2.2278004 0.008021794 1.588165
## 6   67.5  62.2 66.39062 0.1327306  1.1093758 0.006977341 1.589446
##        .cooksd .std.resid
## 1 0.0294637423  2.6994436
## 2 0.0083344662  1.4357182
## 3 0.0008550854 -0.4598700
## 4 0.0048191674 -1.0917327
## 5 0.0080119350 -1.4076640
## 6 0.0017244340  0.7006045</pre>

</article></slide><slide class=''><hgroup><h2>fitted values, Y-hats</h2></hgroup><article  id="fitted-values-y-hats">

<pre class = 'prettyprint lang-r'>library(psych)

describe(galton.data.1$.fitted)      </pre>

<pre >##    vars   n  mean   sd median trimmed  mad   min   max range  skew
## X1    1 928 68.31 0.82  68.34   68.32 0.97 66.23 70.14  3.91 -0.09
##    kurtosis   se
## X1    -0.35 0.03</pre>

<pre class = 'prettyprint lang-r'>describe(galton.data.1$parent)</pre>

<pre >##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05
##      se
## X1 0.06</pre>

</article></slide><slide class=''><hgroup><h2>Correlation</h2></hgroup><article  id="correlation">

<pre class = 'prettyprint lang-r'>cor.test(galton.data.1$parent, galton.data.1$.fitted)</pre>

<pre >## 
##  Pearson&#39;s product-moment correlation
## 
## data:  galton.data.1$parent and galton.data.1$.fitted
## t = 15.711, df = 926, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4064067 0.5081153
## sample estimates:
##       cor 
## 0.4587624</pre>

</article></slide><slide class=''><hgroup><h2>residuals</h2></hgroup><article  id="residuals">

<pre class = 'prettyprint lang-r'>head(galton.data.1$.resid)</pre>

<pre >## [1]  4.2721996  2.2721996 -0.7278004 -1.7278004 -2.2278004  1.1093758</pre>

<pre class = 'prettyprint lang-r'>describe(galton.data.1$.resid)</pre>

<pre >##    vars   n mean   sd median trimmed  mad   min  max range skew kurtosis
## X1    1 928    0 1.59  -0.15    0.02 1.52 -4.67 4.27  8.94 -0.1    -0.17
##      se
## X1 0.05</pre>

<pre class = 'prettyprint lang-r'>describe(galton.data.1$parent)</pre>

<pre >##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05
##      se
## X1 0.06</pre>

<ul>
<li>variation that is left over in Y, after accounting for X</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Statistical Inference</h2></hgroup><article  id="statistical-inference">

<ul>
<li>The way the world is = our model + error</li>
<li>How good is our model? Does it &quot;fit&quot; the data well?</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Partitioning variance in Y</h2></hgroup><article  id="partitioning-variance-in-y">

<ul>
<li>Consider the case with no correlation btw X and Y \[ \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X}) \] \[ \hat{Y} = \bar{Y} \]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Partitioning variance in Y</h2></hgroup><article  id="partitioning-variance-in-y-1">

<ul>
<li>To the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Partitioning variance in Y into 3 components</h2></hgroup><article  id="partitioning-variance-in-y-into-3-components">

<p>\[ Y = \hat{Y} + e\] \[ Y = \hat{Y} + (Y - \hat{Y}) \] \[ Y - \bar{Y} = (\hat{Y} -\bar{Y}) + (Y - \hat{Y}) \] \[ (Y - \bar{Y})^2 = [(\hat{Y} -\bar{Y}) + (Y - \hat{Y})]^2 \] \[ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 \]</p>

</article></slide><slide class=''><hgroup><h2>Partitioning the variation in Y</h2></hgroup><article  id="partitioning-the-variation-in-y">

<p>\[ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 \]</p>

<ul>
<li>SS total = SS regression + SS residual (or error)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Partitioning the variation for ANOVA</h2></hgroup><article  id="partitioning-the-variation-for-anova">

<p>\[ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 \] - SS total = SS between + SS within</p>

</article></slide><slide class=''><hgroup><h2>Similarity with ANOVA</h2></hgroup><article  id="similarity-with-anova">

<p>Example.</p>

<pre class = 'prettyprint lang-r'>t.test(traffic.risk ~ tx, data = ANOVA.example)</pre>

<pre >## 
##  Welch Two Sample t-test
## 
## data:  traffic.risk by tx
## t = 4.9088, df = 214.41, p-value = 1.814e-06
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.2879201 0.6742914
## sample estimates:
## mean in group 0 mean in group 1 
##        2.650641        2.169535</pre>

</article></slide><slide class=''><hgroup><h2>Similarity with ANOVA</h2></hgroup><article  id="similarity-with-anova-1">

<pre class = 'prettyprint lang-r'>model.1&lt;- aov(traffic.risk ~ tx, data = ANOVA.example)
summary(model.1)</pre>

<pre >##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## tx            1   14.8  14.800    24.4 1.38e-06 ***
## Residuals   268  162.6   0.607                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 10 observations deleted due to missingness</pre>

</article></slide><slide class=''><hgroup><h2>Similarity with ANOVA</h2></hgroup><article  id="similarity-with-anova-2">

<pre class = 'prettyprint lang-r'>model.2 &lt;- lm(traffic.risk ~ tx, data = ANOVA.example)
anova(model.2)</pre>

<pre >## Analysis of Variance Table
## 
## Response: traffic.risk
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## tx          1  14.80 14.7999  24.398 1.381e-06 ***
## Residuals 268 162.57  0.6066                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=''><hgroup><h2>Similarity with ANOVA</h2></hgroup><article  id="similarity-with-anova-3">

<pre class = 'prettyprint lang-r'>model.2 &lt;- lm(traffic.risk ~ tx, data = ANOVA.example)
summary(model.2)</pre>

<pre >## 
## Call:
## lm(formula = traffic.risk ~ tx, data = ANOVA.example)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.65064 -0.59811 -0.02668  0.54475  2.54475 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.65064    0.07637  34.707  &lt; 2e-16 ***
## tx          -0.48111    0.09740  -4.939 1.38e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7789 on 268 degrees of freedom
##   (10 observations deleted due to missingness)
## Multiple R-squared:  0.08344,    Adjusted R-squared:  0.08002 
## F-statistic:  24.4 on 1 and 268 DF,  p-value: 1.381e-06</pre>

</article></slide><slide class=''><hgroup><h2>Partitioning variation in Y</h2></hgroup><article  id="partitioning-variation-in-y">

<p>\[ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 \] \[ s_{y}^2 = s_{regression}^2 + s_{residual}^2 \] \[ 1 = \frac{s_{regression}^2}{s_{y}^2} + \frac{s_{residual}^2}{s_{y}^2}  \]</p>

</article></slide><slide class=''><hgroup><h2>Coefficient of Determination</h2></hgroup><article  id="coefficient-of-determination">

<p>\[ \frac{s_{regression}^2}{s_{y}^2} = \frac{SS_{regression}}{SS_{Y}} = R^2 \]</p>

</article></slide><slide class=''><hgroup><h2>Example</h2></hgroup><article  id="example-1">

<pre class = 'prettyprint lang-r'>summary(fit.1)</pre>

<pre >## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***
## child        0.32565    0.02073   15.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105, Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16</pre>

</article></slide><slide class=''><hgroup><h2>Example</h2></hgroup><article  id="example-2">

<pre class = 'prettyprint lang-r'>cor.test(galton.data$parent, galton.data$child)</pre>

<pre >## 
##  Pearson&#39;s product-moment correlation
## 
## data:  galton.data$parent and galton.data$child
## t = 15.711, df = 926, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4064067 0.5081153
## sample estimates:
##       cor 
## 0.4587624</pre>

</article></slide><slide class=''><hgroup><h2>calculating R2</h2></hgroup><article  id="calculating-r2">

<pre >## Analysis of Variance Table
## 
## Response: parent
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## child       1  623.26  623.26  246.84 &lt; 2.2e-16 ***
## Residuals 926 2338.10    2.52                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=''><hgroup><h2>Computing Sum of Squares</h2></hgroup><article  id="computing-sum-of-squares">

<p>\[  \frac{SS_{regression}}{SS_{Y}} = R^2 \] \[  {SS_{regression}} = R^2({SS_{Y})} \] \[  {SS_{residual}} = SS_{Y} - R^2({SS_{Y})} \]</p>

<p>\[  {SS_{residual}} = (1- R^2){SS_{Y}} \]</p>

</article></slide><slide class=''><hgroup><h2>Mean square error (MSE)</h2></hgroup><article  id="mean-square-error-mse">

<ul>
<li>unbiased estimate of error variance</li>
<li>measure of discrepancy between the data and the model</li>
<li>the MSE is the variance around the fitted regression line</li>
</ul>

</article></slide><slide class=''><hgroup><h2>MSE and residual standard error</h2></hgroup><article  id="mse-and-residual-standard-error">

<p>MSE = 2.52</p>

<pre >## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***
## child        0.32565    0.02073   15.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105, Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16</pre>

</article></slide><slide class=''><hgroup><h2>residual standard error (sigma)</h2></hgroup><article  id="residual-standard-error-sigma">

<pre class = 'prettyprint lang-r'>describe(galton.data$parent)</pre>

<pre >##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05
##      se
## X1 0.06</pre>

</article></slide><slide class=''><hgroup><h2>residual standard error (sigma)</h2></hgroup><article  id="residual-standard-error-sigma-1">

<ul>
<li>aka standard deviation of the residual</li>
<li><p>aka standard error of the estimate</p></li>
<li>Interpreted in original units (cf R2)</li>
<li>Standard deviation of Y not accounted by model</li>
<li>MSE is related to sigma, sqrt[(SSE)/df(error)] = sigma</li>
<li><p>(note df might be off slightly because of unbiasing)</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>residual standard error (sigma)</h2></hgroup><article  id="residual-standard-error-sigma-2">

<pre class = 'prettyprint lang-r'>head(galton.data.1)</pre>

<pre >##   parent child  .fitted   .se.fit     .resid        .hat   .sigma
## 1   70.5  61.7 66.22780 0.1423187  4.2721996 0.008021794 1.583599
## 2   68.5  61.7 66.22780 0.1423187  2.2721996 0.008021794 1.588097
## 3   65.5  61.7 66.22780 0.1423187 -0.7278004 0.008021794 1.589686
## 4   64.5  61.7 66.22780 0.1423187 -1.7278004 0.008021794 1.588844
## 5   64.0  61.7 66.22780 0.1423187 -2.2278004 0.008021794 1.588165
## 6   67.5  62.2 66.39062 0.1327306  1.1093758 0.006977341 1.589446
##        .cooksd .std.resid
## 1 0.0294637423  2.6994436
## 2 0.0083344662  1.4357182
## 3 0.0008550854 -0.4598700
## 4 0.0048191674 -1.0917327
## 5 0.0080119350 -1.4076640
## 6 0.0017244340  0.7006045</pre>

<pre class = 'prettyprint lang-r'>describe(galton.data.1$.resid)</pre>

<pre >##    vars   n mean   sd median trimmed  mad   min  max range skew kurtosis
## X1    1 928    0 1.59  -0.15    0.02 1.52 -4.67 4.27  8.94 -0.1    -0.17
##      se
## X1 0.05</pre>

</article></slide><slide class=''><hgroup><h2>Omnibus test</h2></hgroup><article  id="omnibus-test">

<p>\[ H_{0}: \rho_{XY}^2= 0 \] \[ H_{1}: \rho_{XY}^2 \neq 0 \]</p>

<p>\[ F = \frac{MS_{regression}}{MS_{residial}} \]</p>

</article></slide><slide class=''><hgroup><h2>Regression coefficient</h2></hgroup><article  id="regression-coefficient-1">

<p>\[ H_{0}: \beta_{XY}= 0 \] \[ H_{1}: \beta_{XY} \neq 0 \]</p>

</article></slide><slide class=''><hgroup><h2>What does the regression coefficient test?</h2></hgroup><article  id="what-does-the-regression-coefficient-test">

<ul>
<li>Does X provide any predictive information?</li>
<li>Does X provide any explanatory power regarding the variability of Y?</li>
<li>Is the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)</li>
<li>Is the regression line flat?</li>
<li>Are X and Y correlated?</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Regression coefficient</h2></hgroup><article  id="regression-coefficient-2">

<p>\[ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}} \] \[ t(n-2) = \frac{b_{yx}}{se_{b}} \] ** what is standardized equation?</p>

</article></slide><slide class=''><hgroup><h2>Intercept</h2></hgroup><article  id="intercept">

<ul>
<li>same idea, more complex se calculation</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Confidence interval for coefficents</h2></hgroup><article  id="confidence-interval-for-coefficents">

<ul>
<li>same equation as we&#39;ve been working with</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Confidence Bands for regression line</h2></hgroup><article  id="confidence-bands-for-regression-line">

<p><img src="Regression-3_files/figure-html/unnamed-chunk-24-1.png" width="720" /></p>

</article></slide><slide class=''><hgroup><h2>Confidence Bands</h2></hgroup><article  id="confidence-bands">

<ul>
<li>Compare mean estimate for height of 70 based on regression vs binning</li>
</ul>

<p><img src="Regression-3_files/figure-html/unnamed-chunk-25-1.png" width="720" /></p>

</article></slide><slide class=''><hgroup><h2>Confidence Bands</h2></hgroup><article  id="confidence-bands-1">

<p>\[ \hat{Y}\pm t_{critical} * se_{residual}\sqrt{\frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}  \]</p>

</article></slide><slide class=''><hgroup><h2>Prediction</h2></hgroup><article  id="prediction">

<ul>
<li>More later when we start Bayesian modeling</li>
<li>A new Y given x, rather than Ybar given x.</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Coding of qualitative variables</h2></hgroup><article  id="coding-of-qualitative-variables">

<ul>
<li>Dummy coding</li>
<li>Effects</li>
</ul></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
