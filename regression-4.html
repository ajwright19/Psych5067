<!DOCTYPE html>
<html>
<head>
  <title>regression</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'regression',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                      },

      // Author information
      presenters: [
            ]
    };
  </script>

  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

  </style>


</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

  <slide class="title-slide segue nobackground">
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
          </hgroup>
  </slide>

<slide class=""><hgroup><h2>Last time/this time</h2></hgroup><article  id="last-timethis-time">

<ul>
<li>Last time, we looked at associations with two continuous variables</li>
<li>This time, we will extend regression framework to incorporate continuos predictors.</li>
</ul>

</article></slide><slide class=""><hgroup><h2>We want to make eduated guesses</h2></hgroup><article  id="we-want-to-make-eduated-guesses">

<ul>
<li>E(Y|X)</li>
<li>The regression of Y (DV) on X (IV) corresponds to the line that gives the mean value of Y corresponding to each possible value of X</li>
<li>&ldquo;Our best guess&rdquo; regardless of whether our model includes categories or continuous predictor variables</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Regression Equation</h2></hgroup><article  id="regression-equation">

<p>\[ Y = b_{0} + b_{1}X +e  \] \[ \hat{Y} = b_{0} + b_{1}X  \]</p>

</article></slide><slide class=""><hgroup><h2>OLS</h2></hgroup><article  id="ols">

<ul>
<li>How do we find the regression estimates?</li>
<li>Ordinary Least Squares (OLS) estimation</li>
<li>Minimizes deviations</li>
</ul>

<p>\[ min\sum(Y_{i}-\hat{Y})^{2} \]</p>

<ul>
<li>Other estimation procedures possible (and necessary in some cases)</li>
</ul>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section">

<p><img src="regression-4_files/figure-html/unnamed-chunk-2-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-1">

<pre class = 'prettyprint lang-r'>ggplot(d1.f , aes(x=x.1, y=y.1)) +
    geom_point() +
  geom_point(aes(y = .fitted), shape = 1) </pre>

<p><img src="regression-4_files/figure-html/unnamed-chunk-3-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-2">

<pre class = 'prettyprint lang-r'>ggplot(d1.f , aes(x=x.1, y=y.1)) +
    geom_point() +
  geom_point(aes(y = .fitted), shape = 1) +
  geom_segment(aes( xend = x.1, yend = .fitted))</pre>

<p><img src="regression-4_files/figure-html/unnamed-chunk-4-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-3">

<pre class = 'prettyprint lang-r'>ggplot(d1.f , aes(x=x.1, y=y.1)) +
    geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  geom_point(aes(y = .fitted), shape = 1) +
  geom_segment(aes( xend = x.1, yend = .fitted))</pre>

<p><img src="regression-4_files/figure-html/unnamed-chunk-5-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>Regression coefficient</h2></hgroup><article  id="regression-coefficient">

<p>\[ b_{1} = \frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \frac{s_{y}}{s_{x}} \]</p>

</article></slide><slide class=""><hgroup><h2>Interpretation</h2></hgroup><article  id="interpretation">

<ul>
<li>The regression coefficient (slope) equals the estimated change in Y for a 1-unit change in X<br/></li>
<li>For nominal variables using dummy coding (0 and 1s) the regression coefficient equals the difference in means between the two groups</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Standardized regression</h2></hgroup><article  id="standardized-regression">

<ul>
<li>Regression using z-scores for Y and X</li>
<li>Correlation equals standardized regression coefficent \[ b_{1} = r_{xy} \frac{s_{y}}{s_{x}} \] \[ r_{xy} = b_{1} \frac{s_{x}}{s_{y}} \] \[ \beta_{yx} = b_{yx}^{*} = r_{xy} \]</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Standardized regression equation</h2></hgroup><article  id="standardized-regression-equation">

<p>\[ Y = b_{1}^{*}X + e  \] - Interpretation?</p>

</article></slide><slide class=""><hgroup><h2>Raw score regression equation</h2></hgroup><article  id="raw-score-regression-equation">

<ul>
<li>intercept serves to adjust for differences in means between X and Y</li>
</ul>

<p>\[ \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X}) \] - if standardized, intercept drops out<br/>- otherwise, intercept is where regression line crosses the y-axis at X = 0<br/>- Also, notice that when X = \(\bar{X}\) the regression line goes through \(\bar{Y}\)</p>

</article></slide><slide class=""><hgroup><h2>Example</h2></hgroup><article  id="example">

<pre class = 'prettyprint lang-r'>library(psych)
galton.data &lt;- galton
fit.1 &lt;- lm(parent ~ child, data = galton.data)
summary(fit.1)</pre>

<pre >## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***
## child        0.32565    0.02073   15.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105, Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-4">

<p><img src="regression-4_files/figure-html/unnamed-chunk-7-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>ANOVA table</h2></hgroup><article  id="anova-table">

<pre class = 'prettyprint lang-r'>anova(fit.1)</pre>

<pre >## Analysis of Variance Table
## 
## Response: parent
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## child       1  623.26  623.26  246.84 &lt; 2.2e-16 ***
## Residuals 926 2338.10    2.52                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2>Exploring the lm object</h2></hgroup><article  id="exploring-the-lm-object">

<pre class = 'prettyprint lang-r'>library(broom)
galton.data.1 &lt;- augment(fit.1, galton.data)
head(galton.data.1)</pre>

<pre >## # A tibble: 6 x 9
##   parent child .fitted .se.fit .resid    .hat .sigma  .cooksd .std.resid
##    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
## 1   70.5  61.7    66.2   0.142  4.27  0.00802   1.58 0.0295        2.70 
## 2   68.5  61.7    66.2   0.142  2.27  0.00802   1.59 0.00833       1.44 
## 3   65.5  61.7    66.2   0.142 -0.728 0.00802   1.59 0.000855     -0.460
## 4   64.5  61.7    66.2   0.142 -1.73  0.00802   1.59 0.00482      -1.09 
## 5   64    61.7    66.2   0.142 -2.23  0.00802   1.59 0.00801      -1.41 
## 6   67.5  62.2    66.4   0.133  1.11  0.00698   1.59 0.00172       0.701</pre>

</article></slide><slide class=""><hgroup><h2>Y-hats vs Ys</h2></hgroup><article  id="y-hats-vs-ys">

<pre class = 'prettyprint lang-r'>psych::describe(galton.data.1$.fitted)      </pre>

<pre >##    vars   n  mean   sd median trimmed  mad   min   max range  skew
## X1    1 928 68.31 0.82  68.34   68.32 0.97 66.23 70.14  3.91 -0.09
##    kurtosis   se
## X1    -0.35 0.03</pre>

<pre class = 'prettyprint lang-r'>psych::describe(galton.data.1$parent)</pre>

<pre >##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05
##      se
## X1 0.06</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-5">

<p><img src="regression-4_files/figure-html/unnamed-chunk-12-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>residuals</h2></hgroup><article  id="residuals">

<p>\[ \epsilon \sim N(0,\sigma) \]</p>

<pre class = 'prettyprint lang-r'>psych::describe(galton.data.1$.resid)</pre>

<pre >##    vars   n mean   sd median trimmed  mad   min  max range skew kurtosis
## X1    1 928    0 1.59  -0.15    0.02 1.52 -4.67 4.27  8.94 -0.1    -0.17
##      se
## X1 0.05</pre>

<pre class = 'prettyprint lang-r'>psych::describe(galton.data.1$parent)</pre>

<pre >##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05
##      se
## X1 0.06</pre>

<ul>
<li>variation that is left over in Y, after accounting for X</li>
</ul>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-6">

<p><img src="regression-4_files/figure-html/unnamed-chunk-14-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>residuals not assocaited with X</h2></hgroup><article  id="residuals-not-assocaited-with-x">

<pre class = 'prettyprint lang-r'>ggplot(galton.data.1, aes(x=.resid, y=child)) +
    geom_point() +    
    geom_smooth(method=lm)</pre>

<p><img src="regression-4_files/figure-html/unnamed-chunk-15-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>and yhat</h2></hgroup><article  id="and-yhat">

<pre class = 'prettyprint lang-r'>ggplot(galton.data.1, aes(x=.resid, y=.fitted)) +
    geom_point() +    
    geom_smooth(method=lm)</pre>

<p><img src="regression-4_files/figure-html/unnamed-chunk-16-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>r(fitted and X) = 1</h2></hgroup><article  id="rfitted-and-x-1">

<pre class = 'prettyprint lang-r'>ggplot(galton.data.1, aes(x=.fitted, y=child)) +
    geom_point() +    
    geom_smooth(method=lm)</pre>

<p><img src="regression-4_files/figure-html/unnamed-chunk-17-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>Statistical Inference</h2></hgroup><article  id="statistical-inference">

<ul>
<li>The way the world is = our model + error</li>
<li>How good is our model? Does it &ldquo;fit&rdquo; the data well?</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Partitioning variance in Y</h2></hgroup><article  id="partitioning-variance-in-y">

<ul>
<li>Consider the case with no correlation btw X and Y \[ \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X}) \] \[ \hat{Y} = \bar{Y} \]</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Partitioning variance in Y</h2></hgroup><article  id="partitioning-variance-in-y-1">

<ul>
<li>To the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction</li>
</ul>

<p>\[ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 \]</p>

<ul>
<li>SS total = SS regression + SS residual (or error)</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Coefficient of Determination</h2></hgroup><article  id="coefficient-of-determination">

<p>\[ \frac{s_{regression}^2}{s_{y}^2} = \frac{SS_{regression}}{SS_{Y}} = R^2 \]</p>

</article></slide><slide class=""><hgroup><h2>Example</h2></hgroup><article  id="example-1">

<pre class = 'prettyprint lang-r'>summary(fit.1)$r.squared</pre>

<pre >## [1] 0.2104629</pre>

</article></slide><slide class=""><hgroup><h2>Example</h2></hgroup><article  id="example-2">

<pre class = 'prettyprint lang-r'>cor.test(galton.data$parent, galton.data$child)</pre>

<pre >## 
##  Pearson&#39;s product-moment correlation
## 
## data:  galton.data$parent and galton.data$child
## t = 15.711, df = 926, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4064067 0.5081153
## sample estimates:
##       cor 
## 0.4587624</pre>

</article></slide><slide class=""><hgroup><h2>calculating R2</h2></hgroup><article  id="calculating-r2">

<pre >## Analysis of Variance Table
## 
## Response: parent
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## child       1  623.26  623.26  246.84 &lt; 2.2e-16 ***
## Residuals 926 2338.10    2.52                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2>Computing Sum of Squares</h2></hgroup><article  id="computing-sum-of-squares">

<p>\[  \frac{SS_{regression}}{SS_{Y}} = R^2 \] \[  {SS_{regression}} = R^2({SS_{Y})} \] \[  {SS_{residual}} = SS_{Y} - R^2({SS_{Y})} \]</p>

<p>\[  {SS_{residual}} = (1- R^2){SS_{Y}} \]</p>

</article></slide><slide class=""><hgroup><h2>Mean square error (MSE)</h2></hgroup><article  id="mean-square-error-mse">

<ul>
<li>AKA means square residual/within</li>
<li>unbiased estimate of error variance</li>
<li>measure of discrepancy between the data and the model</li>
<li>the MSE is the variance around the fitted regression line</li>
<li>just like MSwithin was variance around predicted group means</li>
</ul>

</article></slide><slide class=""><hgroup><h2>residual standard error</h2></hgroup><article  id="residual-standard-error">

<p>MSE = 2.52</p>

<pre >## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***
## child        0.32565    0.02073   15.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105, Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16</pre>

</article></slide><slide class=""><hgroup><h2>residual standard error/deviation</h2></hgroup><article  id="residual-standard-errordeviation">

<ul>
<li>aka standard deviation of the residual</li>
<li>aka standard error of the estimate</li>
</ul>

<p>\[ \hat{\sigma} \]</p>

<ul>
<li>interpreted in original units (cf R2)</li>
<li>standard deviation of Y not accounted by model</li>
</ul>

</article></slide><slide class=""><hgroup><h2>residual standard error/deviation</h2></hgroup><article  id="residual-standard-errordeviation-1">

<pre class = 'prettyprint lang-r'>summary(fit.1)$sigma </pre>

<pre >## [1] 1.589008</pre>

<pre class = 'prettyprint lang-r'>psych::describe(galton.data.1$.resid)</pre>

<pre >##    vars   n mean   sd median trimmed  mad   min  max range skew kurtosis
## X1    1 928    0 1.59  -0.15    0.02 1.52 -4.67 4.27  8.94 -0.1    -0.17
##      se
## X1 0.05</pre>

</article></slide><slide class=""><hgroup><h2>residual standard error/deviation</h2></hgroup><article  id="residual-standard-errordeviation-2">

<pre class = 'prettyprint lang-r'>summary(fit.1)$sigma </pre>

<pre >## [1] 1.589008</pre>

<pre class = 'prettyprint lang-r'>psych::describe(galton.data$parent)</pre>

<pre >##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05
##      se
## X1 0.06</pre>

</article></slide><slide class=""><hgroup><h2>Why do we care? Let’s simulate to find out</h2></hgroup><article  id="why-do-we-care-lets-simulate-to-find-out">

<pre class = 'prettyprint lang-r'>x.1 &lt;- rnorm(1000, 0, 1)
e.1 &lt;- rnorm(1000, 0, 1)
y.1 &lt;- .5 + .55 * x.1 + e.1
d.1 &lt;- data.frame(x.1,y.1)
m.1 &lt;- lm(y.1 ~ x.1, data = d.1)</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-7">

<pre class = 'prettyprint lang-r'>summary(m.1)</pre>

<pre >## 
## Call:
## lm(formula = y.1 ~ x.1, data = d.1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0280 -0.6871  0.0023  0.7090  3.3020 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.53554    0.03150   17.00   &lt;2e-16 ***
## x.1          0.62992    0.03145   20.03   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.996 on 998 degrees of freedom
## Multiple R-squared:  0.2867, Adjusted R-squared:  0.2859 
## F-statistic: 401.1 on 1 and 998 DF,  p-value: &lt; 2.2e-16</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-8">

<p><img src="regression-4_files/figure-html/unnamed-chunk-26-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-9">

<pre class = 'prettyprint lang-r'>x.2 &lt;- rnorm(1000, 0, 1)
e.2 &lt;- rnorm(1000, 0, 2)
y.2 &lt;- .5 + .55 * x.2 + e.2
d.2 &lt;- data.frame(x.2,y.2)
m.2 &lt;- lm(y.2 ~ x.2, data = d.2)</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-10">

<pre class = 'prettyprint lang-r'>summary(m.2)</pre>

<pre >## 
## Call:
## lm(formula = y.2 ~ x.2, data = d.2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.1107 -1.2806  0.0186  1.3581  5.7829 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.47495    0.06294   7.547 1.01e-13 ***
## x.2          0.64871    0.06418  10.107  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.99 on 998 degrees of freedom
## Multiple R-squared:  0.09285,    Adjusted R-squared:  0.09194 
## F-statistic: 102.2 on 1 and 998 DF,  p-value: &lt; 2.2e-16</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-11">

<p><img src="regression-4_files/figure-html/unnamed-chunk-29-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>r2 and residual standard deviation</h2></hgroup><article  id="r2-and-residual-standard-deviation">

<ul>
<li>two sides of same coin</li>
<li>one in original units, the other standardized</li>
<li>R2 can be tricky because the numerator and denominator can be changed in different ways.</li>
<li>for example if variance in Y is changed but with the same model and residual standard error R2 could decline or increase</li>
</ul>

</article></slide><slide class=""><hgroup><h2>standard errors for b</h2></hgroup><article  id="standard-errors-for-b">

<ul>
<li>represent our uncertainty (noise) in our estimate of the regression coefficient</li>
<li>different from residual standard error/deviation (but proportional to)<br/></li>
<li>much like previously we can take our estimate (b) and put confidence regions around it to get an estimate of what could be &ldquo;possible&rdquo; if we ran the study again<br/></li>
<li>(see equation later)</li>
</ul>

</article></slide><slide class=""><hgroup><h2>data generating process</h2></hgroup><article  id="data-generating-process">

<p>\[ Y_{i} \sim \mathcal{N}(\mu,\,\sigma)\  \] in english: Our DV for individual i is distributed normally with a mean of mu and a standard deviation of sigma<br/><br> - this describes how we think our DVs are generated, and the paramters of interest<br/>- a standard regression model assumes this, but we will see other DGPs such as binomial or poisson that do not<br/>- a different DGP will define what &ldquo;type&rdquo; of regression to use<br/>- for normal, \(\mu\) gets all the focus but \(\sigma\) is just as important</p>

</article></slide><slide class=""><hgroup><h2>Inferential tests</h2></hgroup><article  id="inferential-tests">

</article></slide><slide class=""><hgroup><h2>Omnibus test</h2></hgroup><article  id="omnibus-test">

<p>\[ H_{0}: \rho_{XY}^2= 0 \] \[ H_{1}: \rho_{XY}^2 \neq 0 \]</p>

<p>\[ F = \frac{MS_{regression}}{MS_{residial}} \]</p>

</article></slide><slide class=""><hgroup><h2>model comparison</h2></hgroup><article  id="model-comparison">

<ul>
<li><p>Last semester you looked at different models to see how their F and SSs changed depending on what predictors were included. We can do the same thing with regression models!</p></li>
<li><p>The basic idea is asking how much variance remains unexplained in our model. This &ldquo;left over&rdquo; variance can be contrasted with an alternative model/hypothesis. We can ask does adding a new predictor variable help explain more variance or should we stick wtih a parsimonious model.</p></li>
<li><p>Every model test you do implicitly implies you favoring that over an alternative model, typically the null. This framework allows you to be more flexible and explicit.</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-12">

<pre class = 'prettyprint lang-r'>fit.1 &lt;- lm(parent ~ child, data = galton.data)
fit.0 &lt;- lm(parent ~ 1, data = galton)</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-13">

<pre class = 'prettyprint lang-r'>summary(fit.0)</pre>

<pre >## 
## Call:
## lm(formula = parent ~ 1, data = galton)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.3082 -0.8082  0.1918  1.1918  4.6918 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 68.30819    0.05867    1164   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.787 on 927 degrees of freedom</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-14">

<pre class = 'prettyprint lang-r'>summary(fit.1)</pre>

<pre >## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***
## child        0.32565    0.02073   15.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105, Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-15">

<pre class = 'prettyprint lang-r'>anova(fit.0)</pre>

<pre >## Analysis of Variance Table
## 
## Response: parent
##            Df Sum Sq Mean Sq F value Pr(&gt;F)
## Residuals 927 2961.4  3.1946</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-16">

<pre class = 'prettyprint lang-r'>anova(fit.1)</pre>

<pre >## Analysis of Variance Table
## 
## Response: parent
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## child       1  623.26  623.26  246.84 &lt; 2.2e-16 ***
## Residuals 926 2338.10    2.52                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-17">

<pre class = 'prettyprint lang-r'>anova(fit.1, fit.0)</pre>

<pre >## Analysis of Variance Table
## 
## Model 1: parent ~ child
## Model 2: parent ~ 1
##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    926 2338.1                                  
## 2    927 2961.4 -1   -623.26 246.84 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2>model comparisons</h2></hgroup><article  id="model-comparisons">

<ul>
<li>Model comparisons are redundent with nil/null hypotheses and coefficient tests right now, but will be more flexible down the road.</li>
<li>Key is to start thikning about your implicit alternative models</li>
<li>The ultimate goal would be to create two models that represent two equally plausible theories.</li>
<li>Theory A is made up of components XYZ, whereas theory B has QRS components. You can then ask which theory(model) is better?</li>
</ul>

</article></slide><slide class=""><hgroup><h2>regression coefficient</h2></hgroup><article  id="regression-coefficient-1">

<p>\[ H_{0}: \beta_{1}= 0 \] \[ H_{1}: \beta_{1} \neq 0 \]</p>

</article></slide><slide class=""><hgroup><h2>What does the regression coefficient test?</h2></hgroup><article  id="what-does-the-regression-coefficient-test">

<ul>
<li>Does X provide any predictive information?</li>
<li>Does X provide any explanatory power regarding the variability of Y?</li>
<li>Is the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)</li>
<li>Is the regression line flat?</li>
<li>Are X and Y correlated?</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Regression coefficient</h2></hgroup><article  id="regression-coefficient-2">

<p>\[ se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}} \] \[ t(n-2) = \frac{b_{1}}{se_{b}} \] ** what is standardized equation?</p>

</article></slide><slide class=""><hgroup><h2>Intercept</h2></hgroup><article  id="intercept">

<ul>
<li>same idea, more complex se calculation as the calculation depends on how far the X value (here zero) is away from the mean of X</li>
<li>farther from the mean, less information, thus more uncertainty</li>
<li>we will come back and see this equation later</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Confidence interval for coefficents</h2></hgroup><article  id="confidence-interval-for-coefficents">

<ul>
<li>same equation as we’ve been working with</li>
<li>estimate plus minus 1.96*se</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Confidence Bands for regression line</h2></hgroup><article  id="confidence-bands-for-regression-line">

<pre >## Warning: Removed 19 rows containing non-finite values (stat_smooth).</pre>

<pre >## Warning: Removed 19 rows containing missing values (geom_point).</pre>

<pre >## Warning: Removed 1 rows containing non-finite values (stat_smooth).</pre>

<pre >## Warning: Removed 1 rows containing missing values (geom_point).</pre>

<p><img src="regression-4_files/figure-html/unnamed-chunk-36-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>Confidence Bands</h2></hgroup><article  id="confidence-bands">

<ul>
<li>Compare mean estimate for height of 70 based on regression vs binning</li>
<li>Model uses all data where binning uses much less</li>
</ul>

<p><img src="regression-4_files/figure-html/unnamed-chunk-37-1.png" width="720" /></p>

</article></slide><slide class=""><hgroup><h2>Confidence Bands</h2></hgroup><article  id="confidence-bands-1">

<p>\[ \hat{Y}\pm t_{critical} * se_{residual}*\sqrt{\frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}  \]</p>

</article></slide><slide class=""><hgroup><h2>Prediction</h2></hgroup><article  id="prediction">

<ul>
<li>very similar to confidence bands around regressions line</li>
<li>differences is that we are predicting and individual i’s score, not the Y hat for a particular level of X. (A new Yi given x, rather than Ybar given x)</li>
<li>Because there is greater variation in predicting an individual value rather than a collection of individual values (ie the mean) the prediction band is greater</li>
<li>Combines unknown variability in 1) the estimated mean (as reflected in se of b) 2) peoples scores around mean (residual standard error)</li>
</ul>

<p>\[ \hat{Y}\pm t_{critical} * se_{residual}*\sqrt{1+ \frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}  \]</p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-18">

<pre class = 'prettyprint lang-r'>temp_var &lt;- predict(fit.1, interval=&quot;prediction&quot;)
new_df &lt;- cbind(galton.data, temp_var)
pred &lt;- ggplot(new_df, aes(x=child, y=parent))+
       geom_point() +   
  geom_smooth(method=lm,se=TRUE) +
 geom_ribbon(aes(ymin = lwr, ymax = upr), 
               fill = &quot;blue&quot;, alpha = 0.1)</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-19">

<pre class = 'prettyprint lang-r'>pred</pre>

<p><img src="regression-4_files/figure-html/unnamed-chunk-39-1.png" width="720" /></p></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
