---
title: "Repeated measures & MLM"
output: ioslides_presentation
---
## Expanding our toolkit
- We might want to assess people more than once 
- We might want to assess within groups/nested structures 

## Outline
- Repeated Measures ANOVA (RM ANOVA)
- "Mixed" models aka Multilevel models (MLM), hierarchical linear models (HLM), random-effects models, and more...

## Repeated measures ANOVA
- one-way RMANOVA (within subjects ANOVA)
- can also do a mixed designs (both between and within) sometimes refered to as split-plot

## terminology aside
- SS between-groups and SS within-groups 
- between-subjects and within-subjects designs

## why would we want to do this? 

1. our primary interest may be to study the change of an outcome over time, e.g., a learning effect. 
2. multiple outcomes for each subject allows each subject to be his or her own “control”. This allows us to remove subject-to-subject variation (i.e., individidual differences), and likely increasing power 
<br>
<br>
- When would we *not* want to do this? 

## one way RM ANOVA
- extension of the paired t-test  
- E.g. 
    + A measure before, during and after a intervention  
    + A measure repeated across multiple conditions such as condition A, condition B, and condition C
    + Three or more timepoints (seconds, years, grades)
  
## traditional way (univariate) vs "new" way (multivariate MANOVA)

- different ways to overcome violation of independence
- rabbit hole of complexity that is mostly not worth it...
- because there are newer and better methods
- nevertheless, let us persist 
  
## SS decomposition
- SS between: Deviation of subjects' individual means (across treatments) from the grand mean.
- In the RMANOVA, this is largely uninteresting, as we can pretty much assume that ‘subjects differ’

##

| ID   |wine #1|  #2  | #3  |  #4 | Mean | 
|------|-------|------|-----|-----|------|
| 1    |  2    |  5   |  3  |  3  | 3.25 |  
| 2    |  4    |  6   |  5  |  4  | 4.75 |  
| 3    |  5    |  7   |  4  |  5  | 5.25 |   
| 4    |  3    |  4   |  3  |  4  | 3.5  |  
| 5    |  6    |  7   |  6  |  5  | 6.0  |  
| 6    |  2    |  5   |  4  |  3  | 3.5  |   
| 7    |  4    |  5   |  6  |  4  | 4.75 |
| mean | 3.63  | 5.63 | 4.38| 4.25|      |

## SS decomposition
- SS within: how subjects vary about their own mean 
- Compared to between subjects ANOVA, SS residual (within) is split into 2 different components:
-  SStreatment  
    + As in between subjects ANOVA, comparison of treatment marignal means to grand mean
    + now a part of the within subjects variation
- SS residual  
    + Variability of individuals’ scores about their treatment mean
    + SS residual still is our measure of leftover error variance
  + Smaller error term compared to between subjects 

## post hoc tests
- If the overall ANOVA yields a significant result one can test:  
     + pair-wise comparisons  
     + linear, quadratic trends  

## more complex RM designs
- involve interactions
- involve between person (and/or more within person) variables
- involve multiple SS residual terms

## Interactions

$$ Y_{ijk} = \mu + \eta_{j} + \alpha_{k} + \eta \alpha_{jk}+\varepsilon_{ijk} $$

- The interaction variance contributes to both subjects and A factor SSs
- Decreased power (interaction adds noise to error term)
- May increase sphericity

## Mixed designs

- Between and within factor  
<br>
- Wine tasting by groups (sophomores, sommeliers, souses)  
  + Are some wines rated better? (within)  
  + Do groups rate wine differently (between)  
  + Do sommeliers especially dislike merlot? (within- between subjects interaction)  
- Interactions are now interpretable 

## Problems with RM ANOVA
The sphericity assumption (also known as the homogeneity of variance of differences assumption) assumes the variance of the  differences between any two levels of a within subjects factor (e.g,. condition, time) is equivalent

- Greenhouse-Geisser Epsilon,
 Huynh-Feldt Epsil,
 Pillai’s Trace,
 Wilk’s Lambda,
 Hotelling’s Trace, and
 Roy’s Largest Root

- adjusts df if violated
- but unlikely to find this with observational data

## Problems with RM ANOVA
- complete data, no missing cases (unless you do RM MANOVA)
- spacing is same for all time points (and subjects)
- does not handle continuous data 
- cannot do time varying covariates
- no individaul level trends

## which is why you should use
- MLM, HLM, mixed models, mixed effects, random effects models, etc. 

## Nesting and heirarchy
- students within schools
- observations within people 
- members witin family
- people within counties
- observations within people within classrooms within grades within schools within districts within counties within states
<br>
- ignoring this grouping leads to more unexplained variablity
- innacurate comparisons (e.g. simpson's paradox)

## Example
```{r, message = FALSE}
library(tidyverse)

simp<- frame_data(
  ~ID, ~group,  ~test.score, ~study,
1,1,5,1,
2,1,7,3,
3,2,4,1,
4,2,6,4,
5,3,3,3,
6,3,5,5,
7,4,2,4,
8,4,4,6,
9,5,1,5,
10,5,3,7)
```

##
```{r, echo=FALSE}
ggplot(simp, aes(x=study, y=test.score)) +
    geom_point() +   
    geom_smooth(method=lm,  
                se=FALSE) 
```

## could aggragate across group

```{r}
simp.1<- frame_data(
  ~ID, ~group,  ~test.score, ~study,
  1,1,6,2,
  2,2,5,3,
  3,3,4,4,
  4,4,3,5,
  5,5,2,6)
```

## 

```{r, echo=FALSE}
 ggplot(simp.1, aes(x=study, y=test.score)) +
    geom_point() +    
    geom_smooth(method=lm,   
                se=FALSE) 

```

## what about at the individual level? 

```{r, echo=FALSE}
library(knitr)

ggplot(simp, aes(x=study, y=test.score, group = group)) +
    geom_point() +   
    geom_smooth(method=lm,  
                se=FALSE) 
```

## Aggregating is bad
- Especially when it is easy to take into account
- Cons of aggregating:
    + reduced power  
    + change the unit of analysis and thus change the meaning  
    + more difficult to make inferences  

## Examining the different levels is good

- Extracurricular activity (EA) and time spent studying  

- Between person H1: Do students who participate in EA 		                            			spend more time studying?   

- Within person H2: When a student is participating in EA, do they spend more 			time studying (e.g., in-season vs. offseason)?   
			        	
- Notice that H1 and H2 are independent from one another!

## fixed effects regression
$$ \hat{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+...$$
- parameters are considered fixed, only one value  
- can be thought of as purposefully selected or existing values of a variable; can only generalize to particular values used

## fixed effects regression
- easy to see in ANOVA models

$$ Y_{ij} = \mu + \alpha_{j}+e_{ij}$$
- $\alpha$ is the fixed effect if group j
- we don't randomly select treatments, paradigms are purposely created to test a particular manipulation
- or are they? How many types of sad movies can you come up with? 

## random effects
- Can have random parameters that are not fixed, have many values
- 2 ways to think about random  
    + randomly selected from the population (e.g., stimuli are 3 random depression drugs)
    + random as in they are sampled from some population and thus can vary  
-  random effects means that your parameters are predicted and thus have error associated with them
  

##
- Use fixed effects if
    + The groups are regarded as unique entities
    + If group values are determined by researcher through design or manipulation
    
- Use random effects if
    + Groups regarded as a sample from a larger population
    + Understand group differences
    + Account for more 
    
##
- what does a random intercept mean?   


  $$ {Y}_{ij} = \beta_{0j}  +\varepsilon_{ij} $$
  
  
  $$ {\beta}_{0j} = \gamma_{00} + U_{0j} $$
  
## graphically what does this look like?  

- different intercepts for each j unit, a random effects model.
- repeated assessments of happiness taken daily for two weeks
- think about this as per subject


## putting it together  

  $$ {Y}_{ij} = \beta_{0j}  +\varepsilon_{ij} $$
  
  
  
  
  $$ {\beta}_{0j} = \gamma_{00} + U_{0j}$$  
  
  
  
  $$ {Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij} $$
  - two random terms, two types of variances
  
## Level 1 vs Level 2
- Level 1 is the smallest unit of analysis  
    + students, observations, family members

- Level 2 variables are constant for all level 1 variables that are “nested” in it  
    + schools, counties, families, dyads
  
## Random intercepts, fixed slopes

Level 1:
  $$ {Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{1} + \varepsilon_{ij} $$
  
  
 Level 2:  
  $$ {\beta}_{0j} = \gamma_{00} + U_{0j}$$  
  $$ {\beta}_{1j} = \gamma_{10} $$  
  
  
Putting it together: 
  $$ {Y}_{ij} = \gamma_{00} + \gamma_{10} (X_{1})+ U_{0j}  + \varepsilon_{ij} $$
  
## What does this look like graphically? 
- think of as an individual regressoin for each person
- because intercept are random, people can vary
- because slopes are fixed, people have the same slope
- two types of residuals: 
    + represents how much variability there is in the intercepts from person to person 
    + based on individual scores from their predicted score, much like around the regression line


## Random intercepts, random slopes

Level 1:
  $$ {Y}_{ij} = \beta_{0j}  + \beta_{1j}(X_{1}) + \varepsilon_{ij} $$
  
  
 Level 2:  
  $$ {\beta}_{0j} = \gamma_{00} + U_{0j}$$  
  $$ {\beta}_{1j} = \gamma_{10} + U_{1j} $$  
  
  
Putting it together: 
  $$ {Y}_{ij} = \gamma_{00} + \gamma_{10}(X_{1})+ U_{0j} + U_{1j} (X_{1})+ \varepsilon_{ij} $$
  
## adding covariates and predictors
- can add covariates and predictors at level 1 and level 2
  
## Estimation

- Maximum Likelihood
- Bayesian Estimation

## Data structure

- long vs wide
- use tidyr to convert

## Unconditional model

Level 1: 
  $$ {Y}_{ij} = \beta_{0j}  +\varepsilon_{ij} $$
  
Level 2:
  $$ {\beta}_{0j} = \gamma_{00} + U_{0j}$$  
  
Combined:
  $$ {Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij} $$

ICC: 

$$\frac{U_{0j}}{U_{0j}+ \varepsilon_{ij}}$$

- % variation between vs within group (person) variance



## Example

```{r, echo=FALSE, message=FALSE}
alcohol1 <- read_csv("alcohol1.txt")

library(tidyverse)
library(broom)

alcohol1 <- as_tibble(alcohol1)
```


```{r}
alcohol1
```

##
```{r, message=FALSE, eval=FALSE}
library(lme4)
model.1 <- lmer(alcuse~ 1 + (1 | id), data = alcohol1)
summary(model.1)
```

##
```{r, message=FALSE, echo=FALSE}
library(lme4)
model.1 <- lmer(alcuse~ 1 + (1 | id), data = alcohol1)
summary(model.1)
```

##

```{r, message=FALSE}
library(reghelper)
ICC(model.1)
```

## 
```{r, message = FALSE}
library(sjPlot)
sjp.lmer(model.1, facet.grid = FALSE, 
          sort = "sort.all")
```


##
```{r, echo=FALSE}
alcohol1 <- alcohol1 %>% 
  mutate(time = age-14) 

```


```{r, eval=FALSE}

model.2 <- lmer(alcuse ~ time + (1 | id), data = alcohol1)
summary(model.2)
```


##
```{r, echo=FALSE}

model.2 <- lmer(alcuse ~ time + (1 | id), data = alcohol1)
summary(model.2)
```


##

```{r, message=FALSE}

sjp.lmer(model.2, type="fe")

```


## 

```{r, eval=FALSE}
model.3 <- lmer(alcuse ~ time + (1 + time| id), data = alcohol1)
summary(model.3)

## Fixed effects are outside of the parenthesis 
## and the random effects are inside
```

##
```{r, echo=FALSE}
alcohol1$time <- alcohol1$age -14 
model.3 <- lmer(alcuse ~ time + (1 + time| id), data = alcohol1)
summary(model.3)
```

##
```{r, message=FALSE, eval=FALSE}
library(lmerTest)
summary(model.3)

```

## 
```{r}
sjp.lmer(model.3)
```

##
```{r}
ggplot(alcohol1,
   aes(x = time, y = alcuse, group = id)) + stat_smooth(method = "lm", se = FALSE)
```



##
```{r, message=FALSE, eval=FALSE}

library(lmerTest)
```



##
```{r, message=FALSE, eval=FALSE}
model.4 <- lmer(alcuse~ time + coa + coa*time + (time | id), data = alcohol1)
summary(model.4)
```

##
```{r, message=FALSE, echo=FALSE}
model.4 <- lmer(alcuse~ time + coa + coa*time + (time | id), data = alcohol1)
summary(model.4)
```

##

```{r}
tidy(model.4)
```

## 
```{r, echo=FALSE}
example <- sleepstudy %>%
  mutate(A = ifelse(Days < 5, -1, 1)) %>%
  select(Subject, A, Reaction)
example$A <- as.factor(example$A)

```


what does this look like for group effects? 

##
```{r}
example
```


```{r, echo = FALSE}
base_plot <- ggplot(example, aes(x = A, y = Reaction)) +
  stat_summary(aes(fill = factor(A)), fun.y = mean, geom = "bar", color = "black") 
base_plot
```



##

```{r, eval=FALSE}
ex.1 <- lmer(Reaction ~ 1 + (1|Subject), data = example)
summary(ex.1)
```

##

```{r, echo=FALSE}
ex.1 <- lmer(Reaction ~ 1 + (1|Subject), data = example)
summary(ex.1)
```

## 
```{r}
library(sjPlot)
sjp.lmer(ex.1, facet.grid = FALSE,
          sort = "sort.all")
```


##
```{r}
reghelper::ICC(ex.1)
```

##

```{r, eval=FALSE}
ex.2 <- lmer(Reaction ~ A + (1|Subject), data = example)
summary(ex.2)
```


##

```{r, echo=FALSE}
ex.2 <- lmer(Reaction ~ A + (1|Subject), data = example)
summary(ex.2)
```

## 
how can you think of this graphically? 

##

```{r, eval=FALSE}
ex.3 <- lmer(Reaction ~ A + (A|Subject), data = example)
summary(ex.3)
```

##

```{r, echo=FALSE}
ex.3 <- lmer(Reaction ~ A + (A|Subject), data = example)
summary(ex.3)
```

##
```{r}
ranef(ex.3)
```

##
```{r}
sjp.lmer(ex.3)
```

## 
how can you think of this graphically?
