<!DOCTYPE html>
<html>
<head>
  <title>Logistic Regression</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Logistic Regression',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                      },

      // Author information
      presenters: [
            ]
    };
  </script>

  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

  </style>


</head>

<body style="opacity: 0">

<slides>

  <slide class="title-slide segue nobackground">
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
          </hgroup>
  </slide>

<slide class=""><hgroup><h2>Logistic Regression</h2></hgroup><article  id="logistic-regression">

<p>Used when your DV is binary (0,1)<br/>- Clinical diagnosis<br/>- Disease prevalence<br/>- Experiences (Yes/No)</p>

</article></slide><slide class=""><hgroup><h2>Assumption violations</h2></hgroup><article  id="assumption-violations">

<p>Violates:<br/>- Correctly specified form<br/>- Homoscedasticity<br/>- Normality of the errors</p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section">

<p><img src="Log.1.png" width="750" height="550"/></p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-1">

<p><img src="Log.2.png" width="750" height="550"/></p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-2">

<p><img src="Log.3.png" width="750" height="550"/></p>

</article></slide><slide class=""><hgroup><h2>Need to think in terms of probabilities</h2></hgroup><article  id="need-to-think-in-terms-of-probabilities">

<ul>
<li>If we use OLS, we violate assumptions and have predicted values that go outside 0 &amp; 1<br/></li>
<li>How does the predicted probability of getting a 0 or a 1 relate to our predictors?</li>
</ul>

<p>\[  \hat{p}_{i} \leftrightsquigarrow b_{0} + b_{1}X_{1} + b_{2}X_{2}... b_{3}X_{p} \]</p>

</article></slide><slide class=""><hgroup><h2>Generalized linear models</h2></hgroup><article  id="generalized-linear-models">

<ul>
<li>extend the general linear model framework<br/></li>
<li><p>need to use if the range of Y is restricted (e.g. binary, count) and/or the variance of Y depends on the mean<br/><br></p></li>
<li>made up of two functions<br/></li>
</ul>

<ol>
<li><p>Link Function - describes how the mean depends on the predictors \(g(\mu) = \eta_{i}\)</p></li>
<li><p>Variance Function - describes how the variance depends on the mean \(var(Y)=\phi V(\mu)\)</p></li>
</ol>

</article></slide><slide class=""><hgroup><h2>Error structure</h2></hgroup><article  id="error-structure">

<p>In LMs, we assume that the errors \(ε_{i}\) are independent and identically distributed such that <br> <br> \(E[ε_{i}] = 0\) and \(var[ε_{i}] = s^2\) <br> <br> Typically we assume \(ε_{i}\sim N(0,σ^2)\) as a basis for inference, e.g. t-tests</p>

</article></slide><slide class=""><hgroup><h2>Generalized linear model</h2></hgroup><article  id="generalized-linear-model">

<ul>
<li>need to accomplish two goals:

<ol>
<li>specify a link function<br/></li>
<li>need to specify the error structure</li>
</ol></li>
<li>luckily this handled together</li>
</ul>

</article></slide><slide class=""><hgroup><h2>General Linear Model</h2></hgroup><article  id="general-linear-model">

<ul>
<li><p>GLM (aka regression as you know it) is a special case of Generalized Linear Models</p></li>
<li><p>Link function (describes how the mean depends on the predictors) is \(g(\mu) = \mu\).</p></li>
<li><p>Variance Function (describes how the variance depends on the mean, related to a particular distribution) \(\phi V(\mu) = 1*\sigma^2\)</p></li>
<li><p>in other words, it assumes the gaussian normal distribution</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-3">

<ul>
<li>In some situations (not when working with a dichotomous outcome) an IV variable can be transformed to improve linearity and homogeneity of variance<br/></li>
<li>problems:<br/>+ response variable has changed!<br/>+ transformation must simulateneously improve linearity and homogeneity of variance<br/>+ transformation may not be defined by the boundaries of the sample space</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Link function for logistc</h2></hgroup><article  id="link-function-for-logistc">

<ul>
<li>we need to map (0,1) to \((-\infty, \infty)\)</li>
<li>Logistic regression uses the logistic function to link the predicted probabilities to the predictors<br/></li>
<li>Think of it as a transformation of Y-hats \[g(\mu) = logit(\mu) = log (\frac {\mu_{i}}{1-\mu_{i}})\]</li>
</ul>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-4">

<p>\[ f(x) = \frac{1}{1+e^{-X}} \]</p>

<p><img src="/Users/Jackson/Box Sync/5067 Regression Spring/5067 spring 18/Log.4.png" width="550" height="350"/></p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-5">

<p>\[ f(x) = \frac{1}{1+e^{-X}} \] \[ \hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}} \] - The form of the logistic function is still nonlinear (because probabilities can only range from 0 to 1)<br/>- Solution is to convert probabilities into odds</p>

</article></slide><slide class=""><hgroup><h2>odds</h2></hgroup><article  id="odds">

<ul>
<li>Odds are defined as the probability of being a case divided by the probability of being a noncase</li>
<li>Not bound between 0 and 1</li>
<li>Range from 0 to infinity</li>
<li>less than one is less than 50% probability \[odds = \frac {\hat{p}}{1-\hat{p}}\]</li>
</ul>

<p>\[ probability= \frac{\hat{odds}}{1+\hat{odds}} \]</p>

</article></slide><slide class=""><hgroup><h2>linear probability model</h2></hgroup><article  id="linear-probability-model">

<p>\[ f(x) = \frac{1}{1+e^{-X}} \] \[ \hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}} \]</p>

<p>\[odds = \frac {\hat{p}}{1-\hat{p}}=e^{b_{0}+b_{1}X} \] \[ logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X \]</p>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-6">

<p>\[ logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X \]</p>

<p>-Predicted scores are not dichotomous<br/>-Instead of predicting probabilities directly, we are instead predicting the log of the odds.</p>

</article></slide><slide class=""><hgroup><h2>other common link functions</h2></hgroup><article  id="other-common-link-functions">

<ul>
<li>logit, probit, log-log, poisson, square root…<br/></li>
<li><p>exponetial family of probability functions</p></li>
<li><p>note, just because you have a dichotomous outcome you don’t have to run a logistic regression e.g., you can run a probit regression</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>estimation</h2></hgroup><article  id="estimation">

<ul>
<li>Maximum likelihood</li>
<li>OLS minimizes the errors, which also maximize R<br/></li>
<li>In logistic regression we are not so lucky<br/></li>
<li>Need to rely on Iterative procedure, ML Estimation</li>
<li>Asymptotic standard errors (approximations)<br/></li>
<li>Interpret test statistics as z’s, not t’s<br/></li>
<li>Wald test = chi square with 1 df = z^2 when F(1, infinity)</li>
</ul>

</article></slide><slide class=""><hgroup><h2>GLM in R</h2></hgroup><article  id="glm-in-r">

<pre class = 'prettyprint lang-r'>glm(formula, family = gaussian, data, weights, subset,
    na.action, start = NULL, etastart, mustart, offset, 
    control = glm.control(...), model = TRUE,
  method = ”glm.fit”, x = FALSE, y = TRUE, contrasts = NULL, ...)</pre>

</article></slide><slide class=""><hgroup><h2>Family Argument</h2></hgroup><article  id="family-argument">

<p>The family argument takes (the name of) a family function which specifies:<br/>1. the link function<br/>2. the variance function</p>

<pre class = 'prettyprint lang-r'>glm(y ~ X1+ X2 + X3 , family = binomial, data = dataset)

  binomial(link = &quot;logit&quot;)
  gaussian(link = &quot;identity&quot;)
  Gamma(link = &quot;inverse&quot;)
  inverse.gaussian(link = &quot;1/mu2&quot;)   poisson(link = &quot;log&quot;)</pre>

</article></slide><slide class=""><hgroup><h2>how to interpret</h2></hgroup><article  id="how-to-interpret">

<ul>
<li><p>b1 is the predicted change in the logit for a 1-unit change in X, holding the other predictors constant</p></li>
<li><p>For a one-unit change in X, holding other predictors constant, the odds that Y = 1 changes by \(e^{b_{1}}\)</p></li>
<li><p>e.g,. \(b_{1}\) = .4, \(e^{.4}\) = 1.49</p></li>
<li><p>for fitted values, need to use entire equation \[ \hat{Y} = e^{b_{0}+b_{1}X_{1}}\]</p></li>
<li><p>turn to probabilities by: odds/(1+odds)</p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>example</h2></hgroup><article  id="example">

<pre class = 'prettyprint lang-r'># 0 = premature
mortality</pre>

<pre >## # A tibble: 300 x 3
##    Intelligence_Self Intelligence_Mate premature.d
##                &lt;int&gt;             &lt;int&gt;       &lt;dbl&gt;
##  1                22                19           1
##  2                22                18           1
##  3                21                21           1
##  4                22                17           1
##  5                19                18           1
##  6                19                20           0
##  7                16                18           1
##  8                15                11           0
##  9                16                21           1
## 10                19                22           1
## # ... with 290 more rows</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-7">

<pre class = 'prettyprint lang-r'>death.1 &lt;- lm(premature.d ~ Intelligence_Self , data = mortality)
summary(death.1)</pre>

<pre >## 
## Call:
## lm(formula = premature.d ~ Intelligence_Self, data = mortality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9030  0.1084  0.1538  0.1907  0.3355 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       0.641769   0.098636   6.506 3.25e-10 ***
## Intelligence_Self 0.011357   0.005807   1.956   0.0514 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3745 on 298 degrees of freedom
## Multiple R-squared:  0.01267,    Adjusted R-squared:  0.009362 
## F-statistic: 3.826 on 1 and 298 DF,  p-value: 0.05141</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-8">

<pre class = 'prettyprint lang-r'>death.2 &lt;- glm(premature.d ~ Intelligence_Self , data = mortality)
summary(death.2)</pre>

<pre >## 
## Call:
## glm(formula = premature.d ~ Intelligence_Self, data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9030   0.1084   0.1538   0.1907   0.3355  
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       0.641769   0.098636   6.506 3.25e-10 ***
## Intelligence_Self 0.011357   0.005807   1.956   0.0514 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.1402466)
## 
##     Null deviance: 42.330  on 299  degrees of freedom
## Residual deviance: 41.793  on 298  degrees of freedom
## AIC: 266.05
## 
## Number of Fisher Scoring iterations: 2</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-9">

<pre class = 'prettyprint lang-r'>anova(death.1)</pre>

<pre >## Analysis of Variance Table
## 
## Response: premature.d
##                    Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## Intelligence_Self   1  0.537 0.53653  3.8256 0.05141 .
## Residuals         298 41.793 0.14025                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-10">

<pre class = 'prettyprint lang-r'>anova(death.2)</pre>

<pre >## Analysis of Deviance Table
## 
## Model: gaussian, link: identity
## 
## Response: premature.d
## 
## Terms added sequentially (first to last)
## 
## 
##                   Df Deviance Resid. Df Resid. Dev
## NULL                                299     42.330
## Intelligence_Self  1  0.53653       298     41.793</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-11">

<pre class = 'prettyprint lang-r'>death.3 &lt;- glm(premature.d ~ Intelligence_Self,
               family = binomial, data = mortality)
summary(death.3)</pre>

<pre >## 
## Call:
## glm(formula = premature.d ~ Intelligence_Self, family = binomial, 
##     data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1175   0.4923   0.5716   0.6438   0.9943  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)        0.28695    0.67490   0.425   0.6707  
## Intelligence_Self  0.08012    0.04143   1.934   0.0532 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 273.53  on 299  degrees of freedom
## Residual deviance: 269.75  on 298  degrees of freedom
## AIC: 273.75
## 
## Number of Fisher Scoring iterations: 4</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-12">

<pre class = 'prettyprint lang-r'>exp(1)^.08012</pre>

<pre >## [1] 1.083417</pre>

<pre class = 'prettyprint lang-r'>#For every 1 unit increase in Intelligence 
#the odds of living increase by 8%.

#prob = 
1.083417/(1+1.083417)</pre>

<pre >## [1] 0.5200193</pre>

<pre class = 'prettyprint lang-r'>exp(1)^(.08012*20 + 0.28695)</pre>

<pre >## [1] 6.615067</pre>

<pre class = 'prettyprint lang-r'>6.615067/ (1+6.615067)</pre>

<pre >## [1] 0.8686814</pre>

</article></slide><slide class=""><hgroup><h2></h2></hgroup><article  id="section-13">

<pre class = 'prettyprint lang-r'>death.4 &lt;- glm(premature.d ~ Intelligence_Self,
        family = binomial(link = &quot;probit&quot;), data = mortality)
summary(death.4)</pre>

<pre >## 
## Call:
## glm(formula = premature.d ~ Intelligence_Self, family = binomial(link = &quot;probit&quot;), 
##     data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1265   0.4889   0.5723   0.6454   0.9750  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)        0.21961    0.38376   0.572   0.5671  
## Intelligence_Self  0.04513    0.02319   1.946   0.0516 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 273.53  on 299  degrees of freedom
## Residual deviance: 269.72  on 298  degrees of freedom
## AIC: 273.72
## 
## Number of Fisher Scoring iterations: 4</pre>

</article></slide><slide class=""><hgroup><h2>count data</h2></hgroup><article  id="count-data">

<pre class = 'prettyprint lang-r'>num.rejects &lt;- glm(rejections ~ Intelligence_SelF, 
                   family=poisson, data = mortality)
summary(num.rejects)</pre></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
